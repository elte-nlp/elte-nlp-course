@article{pashevich2021episodic,
  title={Episodic Transformer for Vision-and-Language Navigation},
  author={Pashevich, Alexander and Schmid, Cordelia and Sun, Chen},
  journal={arXiv preprint arXiv:2105.06453},
  year={2021},
  url={https://arxiv.org/abs/2105.06453},
}

@inproceedings{shridhar2020alfred,
  title={{ALFRED}: A benchmark for interpreting grounded instructions for everyday tasks},
  author={Shridhar, Mohit and Thomason, Jesse and Gordon, Daniel and Bisk, Yonatan and Han, Winson and Mottaghi, Roozbeh and Zettlemoyer, Luke and Fox, Dieter},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={10740--10749},
  year={2020},
  url={https://arxiv.org/abs/1912.01734}
}

@article{jiang2020can,
  title={How can we know what language models know?},
  author={Jiang, Zhengbao and Xu, Frank F and Araki, Jun and Neubig, Graham},
  journal={Transactions of the {Association for Computational Linguistics}},
  volume={8},
  pages={423--438},
  year={2020},
  publisher={MIT Press}
}


@article{shin2020autoprompt,
  title={Autoprompt: Eliciting knowledge from language models with automatically generated prompts},
  author={Shin, Taylor and Razeghi, Yasaman and Logan IV, Robert L and Wallace, Eric and Singh, Sameer},
  journal={arXiv preprint arXiv:2010.15980},
  year={2020}
}


@article{gao2020making,
  title={Making pre-trained language models better few-shot learners},
  author={Gao, Tianyu and Fisch, Adam and Chen, Danqi},
  journal={arXiv preprint arXiv:2012.15723},
  year={2020}
}

@inproceedings{davison2019commonsense,
  title={Commonsense knowledge mining from pretrained models},
  author={Davison, Joe and Feldman, Joshua and Rush, Alexander M},
  booktitle={{EMNLP-IJCNLP}},
  pages={1173--1178},
  year={2019}
}

@article{li2021prefix,
  title={Prefix-tuning: Optimizing continuous prompts for generation},
  author={Li, Xiang Lisa and Liang, Percy},
  journal={arXiv preprint arXiv:2101.00190},
  year={2021}
}

@online{simon2021large,
  author = {Simon, Julien},
  title = {Large Language Models: A New {Moore's} Law?},
  year = {2021},
  url = {https://huggingface.co/blog/large-language-models},
  titleaddon = {Hugging Face blog post}  
}


@article{schick2020few,
  title={Few-shot text generation with pattern-exploiting training},
  author={Schick, Timo and Sch{\"u}tze, Hinrich},
  journal={arXiv preprint arXiv:2012.11926},
  year={2020}
}



@article{liu2021makes,
  title={What Makes Good In-Context Examples for GPT-3?},
  author={Liu, Jiachang and Shen, Dinghan and Zhang, Yizhe and Dolan, Bill and Carin, Lawrence and Chen, Weizhu},
  journal={arXiv preprint arXiv:2101.06804},
  year={2021}
}


@article{lu2021fantastically,
  title={Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity},
  author={Lu, Yao and Bartolo, Max and Moore, Alastair and Riedel, Sebastian and Stenetorp, Pontus},
  journal={arXiv preprint arXiv:2104.08786},
  year={2021}
}

@online{openai2022aligning,
  author= {{OpenAI}},
  title = {Aligning Language Models to Follow Instructions},
  year = {2022},
  url = {https://openai.com/blog/instruction-following/},
  titleaddon = {{OpenAI} blog post}  
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Preprint},
  year={2022}
}

@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  year={2018}
}

@article{wolf2019build,
  title={How to build a State-of-the-Art Conversational {AI} with Transfer Learning},
  author={Wolf, Thomas},
  titleaddon={Hugging Face blog post},
  year={2019}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@misc{epoch2022trendsintrainingdatasetsizes,
  title = "Trends in Training Dataset Sizes",
  author = {Pablo Villalobos and Anson Ho},
  year = 2022,
  url = {https://epochai.org/blog/trends-in-training-dataset-sizes},
  note = "Accessed: 2023-7-23"
}

@article{zaheer2020big,
  title={Big bird: Transformers for longer sequences},
  author={Zaheer, Manzil and Guruganesh, Guru and Dubey, Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and others},
  journal={arXiv preprint arXiv:2007.14062},
  year={2020},
  url={https://arxiv.org/pdf/2007.14062.pdf}
}

@article{liu2021makes,
  title={What Makes Good In-Context Examples for GPT-$3 $?},
  author={Liu, Jiachang and Shen, Dinghan and Zhang, Yizhe and Dolan, Bill and Carin, Lawrence and Chen, Weizhu},
  journal={arXiv preprint arXiv:2101.06804},
  year={2021}
}
@misc{rubin2022learning,
      title={Learning To Retrieve Prompts for In-Context Learning}, 
      author={Ohad Rubin and Jonathan Herzig and Jonathan Berant},
      year={2022},
      eprint={2112.08633},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{weng2023prompt,
  title   = "Prompt Engineering",
  author  = "Weng, Lilian",
  journal = "lilianweng.github.io",
  year    = "2023",
  month   = "Mar",
  url     = "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/"
}

@misc{press2023measuring,
      title={Measuring and Narrowing the Compositionality Gap in Language Models}, 
      author={Ofir Press and Muru Zhang and Sewon Min and Ludwig Schmidt and Noah A. Smith and Mike Lewis},
      year={2023},
      eprint={2210.03350},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{dietterich2000ensemble,
  title={Ensemble methods in machine learning},
  author={Dietterich, Thomas G},
  booktitle={International workshop on multiple classifier systems},
  pages={1--15},
  year={2000},
  organization={Springer}
}

@article{jacobs1991adaptive,
  title={Adaptive mixtures of local experts},
  author={Jacobs, Robert A and Jordan, Michael I and Nowlan, Steven J and Hinton, Geoffrey E},
  journal={Neural computation},
  volume={3},
  number={1},
  pages={79--87},
  year={1991},
  publisher={MIT Press}
}

@misc{shazeer2017outrageously,
      title={Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer}, 
      author={Noam Shazeer and Azalia Mirhoseini and Krzysztof Maziarz and Andy Davis and Quoc Le and Geoffrey Hinton and Jeff Dean},
      year={2017},
      eprint={1701.06538},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{belcak2023fast,
      title={Fast Feedforward Networks}, 
      author={Peter Belcak and Roger Wattenhofer},
      year={2023},
      eprint={2308.14711},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{wang2022image,
      title={Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks}, 
      author={Wenhui Wang and Hangbo Bao and Li Dong and Johan Bjorck and Zhiliang Peng and Qiang Liu and Kriti Aggarwal and Owais Khan Mohammed and Saksham Singhal and Subhojit Som and Furu Wei},
      year={2022},
      eprint={2208.10442},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{chen2022understanding,
      title={Towards Understanding Mixture of Experts in Deep Learning}, 
      author={Zixiang Chen and Yihe Deng and Yue Wu and Quanquan Gu and Yuanzhi Li},
      year={2022},
      eprint={2208.02813},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{anonymous2023mole,
title={Mo{LE}: Mixture of Lo{RA} Experts},
author={Anonymous},
booktitle={Submitted to The Twelfth International Conference on Learning Representations},
year={2023},
url={https://openreview.net/forum?id=uWvKBCYh4S},
note={under review}
}

@misc{wu2023pituning,
      title={$\pi$-Tuning: Transferring Multimodal Foundation Models with Optimal Multi-task Interpolation}, 
      author={Chengyue Wu and Teng Wang and Yixiao Ge and Zeyu Lu and Ruisong Zhou and Ying Shan and Ping Luo},
      year={2023},
      eprint={2304.14381},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{zadouri2023pushing,
      title={Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning}, 
      author={Ted Zadouri and Ahmet Üstün and Arash Ahmadian and Beyza Ermiş and Acyr Locatelli and Sara Hooker},
      year={2023},
      eprint={2309.05444},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{liu2023moelora,
      title={MOELoRA: An MOE-based Parameter Efficient Fine-Tuning Method for Multi-task Medical Applications}, 
      author={Qidong Liu and Xian Wu and Xiangyu Zhao and Yuanshao Zhu and Derong Xu and Feng Tian and Yefeng Zheng},
      year={2023},
      eprint={2310.18339},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{fedus2022switch,
      title={Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity}, 
      author={William Fedus and Barret Zoph and Noam Shazeer},
      year={2022},
      eprint={2101.03961},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{belcak2023exponentially,
      title={Exponentially Faster Language Modelling}, 
      author={Peter Belcak and Roger Wattenhofer},
      year={2023},
      eprint={2311.10770},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
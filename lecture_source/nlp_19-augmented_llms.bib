@inproceedings{dudek2023learning,
  title={Learning Sparse Lexical Representations Over Specified Vocabularies for Retrieval},
  author={Dudek, Jeffrey M and Kong, Weize and Li, Cheng and Zhang, Mingyang and Bendersky, Michael},
  booktitle={Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
  pages={3865--3869},
  year={2023}
}

@misc{kamalloo2023resourcesbrewingbeirreproducible,
      title={Resources for Brewing BEIR: Reproducible Reference Models and an Official Leaderboard}, 
      author={Ehsan Kamalloo and Nandan Thakur and Carlos Lassance and Xueguang Ma and Jheng-Hong Yang and Jimmy Lin},
      year={2023},
      eprint={2306.07471},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2306.07471}, 
}

@misc{nogueira2019documentexpansionqueryprediction,
      title={Document Expansion by Query Prediction}, 
      author={Rodrigo Nogueira and Wei Yang and Jimmy Lin and Kyunghyun Cho},
      year={2019},
      eprint={1904.08375},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/1904.08375}, 
}
@misc{nogueira2020passagererankingbert,
      title={Passage Re-ranking with BERT}, 
      author={Rodrigo Nogueira and Kyunghyun Cho},
      year={2020},
      eprint={1901.04085},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/1901.04085}, 
}

@misc{thakur2021beirheterogenousbenchmarkzeroshot,
      title={BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models}, 
      author={Nandan Thakur and Nils Reimers and Andreas Rücklé and Abhishek Srivastava and Iryna Gurevych},
      year={2021},
      eprint={2104.08663},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2104.08663}, 
}

@misc{zhao2020spartaefficientopendomainquestion,
      title={SPARTA: Efficient Open-Domain Question Answering via Sparse Transformer Matching Retrieval}, 
      author={Tiancheng Zhao and Xiaopeng Lu and Kyusong Lee},
      year={2020},
      eprint={2009.13013},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2009.13013}, 
}

@misc{santhanam2022colbertv2effectiveefficientretrieval,
      title={ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction}, 
      author={Keshav Santhanam and Omar Khattab and Jon Saad-Falcon and Christopher Potts and Matei Zaharia},
      year={2022},
      eprint={2112.01488},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2112.01488}, 
}

@misc{pradeep2021expandomonoduodesignpatterntext,
      title={The Expando-Mono-Duo Design Pattern for Text Ranking with Pretrained Sequence-to-Sequence Models}, 
      author={Ronak Pradeep and Rodrigo Nogueira and Jimmy Lin},
      year={2021},
      eprint={2101.05667},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2101.05667}, 
}

@misc{khattab2020colbertefficienteffectivepassage,
      title={ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT}, 
      author={Omar Khattab and Matei Zaharia},
      year={2020},
      eprint={2004.12832},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2004.12832}, 
}

@misc{neelakantan2022textcodeembeddingscontrastive,
      title={Text and Code Embeddings by Contrastive Pre-Training}, 
      author={Arvind Neelakantan and Tao Xu and Raul Puri and Alec Radford and Jesse Michael Han and Jerry Tworek and Qiming Yuan and Nikolas Tezak and Jong Wook Kim and Chris Hallacy and Johannes Heidecke and Pranav Shyam and Boris Power and Tyna Eloundou Nekoul and Girish Sastry and Gretchen Krueger and David Schnurr and Felipe Petroski Such and Kenny Hsu and Madeleine Thompson and Tabarak Khan and Toki Sherbakov and Joanne Jang and Peter Welinder and Lilian Weng},
      year={2022},
      eprint={2201.10005},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2201.10005}, 
}

@misc{ni2021sentencet5scalablesentenceencoders,
      title={Sentence-T5: Scalable Sentence Encoders from Pre-trained Text-to-Text Models}, 
      author={Jianmo Ni and Gustavo Hernández Ábrego and Noah Constant and Ji Ma and Keith B. Hall and Daniel Cer and Yinfei Yang},
      year={2021},
      eprint={2108.08877},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2108.08877}, 
}

@misc{devlin2019bertpretrainingdeepbidirectional,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1810.04805}, 
}

@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year={2018},
  publisher={San Francisco, CA, USA}
}

@misc{qiao2019understandingbehaviorsbertranking,
      title={Understanding the Behaviors of BERT in Ranking}, 
      author={Yifan Qiao and Chenyan Xiong and Zhenghao Liu and Zhiyuan Liu},
      year={2019},
      eprint={1904.07531},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/1904.07531}, 
}

@misc{reimers2019sentencebertsentenceembeddingsusing,
      title={Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks}, 
      author={Nils Reimers and Iryna Gurevych},
      year={2019},
      eprint={1908.10084},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1908.10084}, 
}

@misc{humeau2020polyencoderstransformerarchitecturespretraining,
      title={Poly-encoders: Transformer Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring}, 
      author={Samuel Humeau and Kurt Shuster and Marie-Anne Lachaux and Jason Weston},
      year={2020},
      eprint={1905.01969},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1905.01969}, 
}

@Inbook{Paaß2023,
author="Paa{\ss}, Gerhard
and Giesselbach, Sven",
title="Foundation Models for Text Generation",
bookTitle="Foundation Models for Natural Language Processing: Pre-trained Language Models Integrating Media",
year="2023",
publisher="Springer International Publishing",
address="Cham",
pages="227--311",
abstract="This chapter discusses Foundation Models for Text Generation. This includes systems for Document Retrieval, which accept a query and return an ordered list of text documents from a document collection, often evaluating the similarity of embeddings to retrieve relevant text passages. Question Answering systems are given a natural language question and must provide an answer, usually in natural language. Machine Translation models take a text in one language and translate it into another language. Text Summarization systems receive a long document and generate a short summary covering the most important contents of the document. Text Generation models use an autoregressive Language Model to generate a longer story, usually starting from an initial text input. Dialog systems have the task of conducting a dialog with a human partner, typically not limited to a specific topic.",
isbn="978-3-031-23190-2",
doi="10.1007/978-3-031-23190-2_6",
url="https://doi.org/10.1007/978-3-031-23190-2_6"
}
@misc{sebastien,
  author = {Sebastian Hoefstatter},
  title = {{Advanced Information Retrieval course}},
  howpublished = "\url{https://github.com/sebastian-hofstaetter/teaching/tree/master/advanced-information-retrieval}",
  year = {2021},
  note = "[Online; accessed 19-February-2025]"
}

@misc{gao2021coilrevisitexactlexical,
      title={COIL: Revisit Exact Lexical Match in Information Retrieval with Contextualized Inverted List}, 
      author={Luyu Gao and Zhuyun Dai and Jamie Callan},
      year={2021},
      eprint={2104.07186},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2104.07186}, 
}

@misc{henderson2017efficientnaturallanguageresponse,
      title={Efficient Natural Language Response Suggestion for Smart Reply}, 
      author={Matthew Henderson and Rami Al-Rfou and Brian Strope and Yun-hsuan Sung and Laszlo Lukacs and Ruiqi Guo and Sanjiv Kumar and Balint Miklos and Ray Kurzweil},
      year={2017},
      eprint={1705.00652},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1705.00652}, 
}

@misc{karpukhin2020densepassageretrievalopendomain,
      title={Dense Passage Retrieval for Open-Domain Question Answering}, 
      author={Vladimir Karpukhin and Barlas Oğuz and Sewon Min and Patrick Lewis and Ledell Wu and Sergey Edunov and Danqi Chen and Wen-tau Yih},
      year={2020},
      eprint={2004.04906},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2004.04906}, 
}

@misc{qu2021rocketqaoptimizedtrainingapproach,
      title={RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering}, 
      author={Yingqi Qu and Yuchen Ding and Jing Liu and Kai Liu and Ruiyang Ren and Wayne Xin Zhao and Daxiang Dong and Hua Wu and Haifeng Wang},
      year={2021},
      eprint={2010.08191},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2010.08191}, 
}

@misc{gao2021scalingdeepcontrastivelearning,
      title={Scaling Deep Contrastive Learning Batch Size under Memory Limited Setup}, 
      author={Luyu Gao and Yunyi Zhang and Jiawei Han and Jamie Callan},
      year={2021},
      eprint={2101.06983},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2101.06983}, 
}
@misc{jiang2023scalingsentenceembeddingslarge,
      title={Scaling Sentence Embeddings with Large Language Models}, 
      author={Ting Jiang and Shaohan Huang and Zhongzhi Luan and Deqing Wang and Fuzhen Zhuang},
      year={2023},
      eprint={2307.16645},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2307.16645}, 
}

@misc{muennighoff2022sgptgptsentenceembeddings,
      title={SGPT: GPT Sentence Embeddings for Semantic Search}, 
      author={Niklas Muennighoff},
      year={2022},
      eprint={2202.08904},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2202.08904}, 
}

@misc{springer2024repetitionimproveslanguagemodel,
      title={Repetition Improves Language Model Embeddings}, 
      author={Jacob Mitchell Springer and Suhas Kotha and Daniel Fried and Graham Neubig and Aditi Raghunathan},
      year={2024},
      eprint={2402.15449},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.15449}, 
}

@misc{behnamghader2024llm2veclargelanguagemodels,
      title={LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders}, 
      author={Parishad BehnamGhader and Vaibhav Adlakha and Marius Mosbach and Dzmitry Bahdanau and Nicolas Chapados and Siva Reddy},
      year={2024},
      eprint={2404.05961},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.05961}, 
}

@misc{doshi2024mistralspladellmsbetterlearned,
      title={Mistral-SPLADE: LLMs for better Learned Sparse Retrieval}, 
      author={Meet Doshi and Vishwajeet Kumar and Rudra Murthy and Vignesh P and Jaydeep Sen},
      year={2024},
      eprint={2408.11119},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2408.11119}, 
}

@misc{zhuang2024promptrepspromptinglargelanguage,
      title={PromptReps: Prompting Large Language Models to Generate Dense and Sparse Representations for Zero-Shot Document Retrieval}, 
      author={Shengyao Zhuang and Xueguang Ma and Bevan Koopman and Jimmy Lin and Guido Zuccon},
      year={2024},
      eprint={2404.18424},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2404.18424}, 
}

@misc{nie2024textworthtokenstext,
      title={A Text is Worth Several Tokens: Text Embedding from LLMs Secretly Aligns Well with The Key Tokens}, 
      author={Zhijie Nie and Richong Zhang and Zhanyu Wu},
      year={2024},
      eprint={2406.17378},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.17378}, 
}

@misc{sasazawa2023textretrievalmultistagereranking,
      title={Text Retrieval with Multi-Stage Re-Ranking Models}, 
      author={Yuichi Sasazawa and Kenichi Yokote and Osamu Imaichi and Yasuhiro Sogawa},
      year={2023},
      eprint={2311.07994},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2311.07994}, 
}
@inproceedings{rrf_fusion,
author = {Cormack, Gordon V. and Clarke, Charles L A and Buettcher, Stefan},
title = {Reciprocal rank fusion outperforms condorcet and individual rank learning methods},
year = {2009},
isbn = {9781605584836},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1571941.1572114},
doi = {10.1145/1571941.1572114},
abstract = {Reciprocal Rank Fusion (RRF), a simple method for combining the document rankings from multiple IR systems, consistently yields better results than any individual system, and better results than the standard method Condorcet Fuse. This result is demonstrated by using RRF to combine the results of several TREC experiments, and to build a meta-learner that ranks the LETOR 3 dataset better than any previously reported method},
booktitle = {Proceedings of the 32nd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {758–759},
numpages = {2},
keywords = {ranking, fusion, aggregation},
location = {Boston, MA, USA},
series = {SIGIR '09}
}

@misc{Michelangiolo,
  author = {Michelangiolo Mazzeschi},
  title = {{DBSF: Distribution-Based Score Fusion algorithm}},
  howpublished = "\url{https://github.com/atlantis-nova/hybrid-dbsf}",
  year = {2023},
  note = "[Online; accessed 12-March-2025]"
}

@misc{kim2024autoragautomatedframeworkoptimization,
      title={AutoRAG: Automated Framework for optimization of Retrieval Augmented Generation Pipeline}, 
      author={Dongkyu Kim and Byoungwook Kim and Donggeon Han and Matouš Eibich},
      year={2024},
      eprint={2410.20878},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.20878}, 
}

@inproceedings{borgeaud2022improving,
  title={Improving language models by retrieving from trillions of tokens},
  author={Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie and Van Den Driessche, George Bm and Lespiau, Jean-Baptiste and Damoc, Bogdan and Clark, Aidan and others},
  booktitle={International conference on machine learning},
  pages={2206--2240},
  year={2022},
  organization={PMLR}
}

@misc{gao2022precise,
      title={Precise Zero-Shot Dense Retrieval without Relevance Labels}, 
      author={Luyu Gao and Xueguang Ma and Jimmy Lin and Jamie Callan},
      year={2022},
      eprint={2212.10496},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}

@inproceedings{guu2020retrieval,
  title={Retrieval augmented language model pre-training},
  author={Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Mingwei},
  booktitle={International conference on machine learning},
  pages={3929--3938},
  year={2020},
  organization={PMLR}
}

@misc{qin2023toolllm,
      title={ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs}, 
      author={Yujia Qin and Shihao Liang and Yining Ye and Kunlun Zhu and Lan Yan and Yaxi Lu and Yankai Lin and Xin Cong and Xiangru Tang and Bill Qian and Sihan Zhao and Runchu Tian and Ruobing Xie and Jie Zhou and Mark Gerstein and Dahai Li and Zhiyuan Liu and Maosong Sun},
      year={2023},
      eprint={2307.16789},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{yang2023autogpt,
      title={Auto-GPT for Online Decision Making: Benchmarks and Additional Opinions}, 
      author={Hui Yang and Sifu Yue and Yunzhong He},
      year={2023},
      eprint={2306.02224},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{zheng2024stepbackevokingreasoning,
      title={Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models}, 
      author={Huaixiu Steven Zheng and Swaroop Mishra and Xinyun Chen and Heng-Tze Cheng and Ed H. Chi and Quoc V Le and Denny Zhou},
      year={2024},
      eprint={2310.06117},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.06117}, 
}

@misc{Siegler,
  author = {Ryan Siegler},
  title = {{Optimizing Vector Search with Metadata Filtering and Fuzzy Filtering}},
  howpublished = "\url{https://medium.com/kx-systems/optimizing-vector-search-with-metadata-filtering-41276e1a7370}",
  year = {2024},
  note = "[Online; accessed 14-March-2025]"
}

@misc{Siegler,
  author = {Ryan Siegler},
  title = {{Optimizing Vector Search with Metadata Filtering and Fuzzy Filtering}},
  howpublished = "\url{https://medium.com/kx-systems/optimizing-vector-search-with-metadata-filtering-41276e1a7370}",
  year = {2024},
  note = "[Online; accessed 14-March-2025]"
}
@misc{Mohamed,
  author = {Mohamed},
  title = {{RAG Evaluation Metrics Explained: A Complete Guide}},
  howpublished = "\url{https://medium.com/@med.el.harchaoui/rag-evaluation-metrics-explained-a-complete-guide-dbd7a3b571a8}",
  year = {2024},
  note = "[Online; accessed 14-March-2025]"
}

@misc{es2023ragasautomatedevaluationretrieval,
      title={RAGAS: Automated Evaluation of Retrieval Augmented Generation}, 
      author={Shahul Es and Jithin James and Luis Espinosa-Anke and Steven Schockaert},
      year={2023},
      eprint={2309.15217},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.15217}, 
}

@misc{muennighoff2023mtebmassivetextembedding,
      title={MTEB: Massive Text Embedding Benchmark}, 
      author={Niklas Muennighoff and Nouamane Tazi and Loïc Magne and Nils Reimers},
      year={2023},
      eprint={2210.07316},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2210.07316}, 
}

@article{gao2021simcse,
  title={Simcse: Simple contrastive learning of sentence embeddings},
  author={Gao, Tianyu and Yao, Xingcheng and Chen, Danqi},
  journal={arXiv preprint arXiv:2104.08821},
  year={2021}
}

@article{su2022one,
  title={One embedder, any task: Instruction-finetuned text embeddings},
  author={Su, Hongjin and Kasai, Jungo and Wang, Yizhong and Hu, Yushi and Ostendorf, Mari and Yih, Wen-tau and Smith, Noah A and Zettlemoyer, Luke and Yu, Tao and others},
  journal={arXiv preprint arXiv:2212.09741},
  year={2022}
}




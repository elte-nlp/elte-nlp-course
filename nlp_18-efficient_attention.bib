@misc{chen2023extending,
  title={Extending context window of large language models via positional interpolation},
  author={Chen, Shouyuan and Wong, Sherman and Chen, Liangjian and Tian, Yuandong},
  eprint={2306.15595},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  year={2023}
}

@misc{child2019generating,
  title={Generating long sequences with sparse transformers},
  author={Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  eprint={1904.10509},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  year={2019}
}

@inproceedings{dao2022flashattention,
  title={Flashattention: Fast and memory-efficient exact attention with io-awareness},
  author={Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  booktitle={Advances in Neural Information Processing Systems},
  volume={35},
  pages={16344--16359},
  year={2022}
}

@misc{dao2023flashattention,
  title={Flashattention-2: Faster attention with better parallelism and work partitioning},
  author={Dao, Tri},
  eprint={2307.08691},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  year={2023}
}

@article{fedus2022switch,
  title={Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={The Journal of Machine Learning Research},
  volume={23},
  number={1},
  pages={5232--5270},
  year={2022},
  publisher={JMLRORG}
}

@inproceedings{ press2022train,
  title = {Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation},
  author = {Ofir Press and Noah Smith and Mike Lewis},
  booktitle = {International Conference on Learning Representations},
  year = {2022},
  url = {https://openreview.net/forum?id=R8sQPpGCv0}
}

@inproceedings{xiong-etal-2022-simple,
    title = "Simple Local Attentions Remain Competitive for Long-Context Tasks",
    author = "Xiong, Wenhan  and
      Oguz, Barlas  and
      Gupta, Anchit  and
      Chen, Xilun  and
      Liskovich, Diana  and
      Levy, Omer  and
      Yih, Scott  and
      Mehdad, Yashar",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.144",
    doi = "10.18653/v1/2022.naacl-main.144",
    pages = "1975--1986"
}

@article{zaheer2020big,
  title={Big bird: Transformers for longer sequences},
  author={Zaheer, Manzil and Guruganesh, Guru and Dubey, Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and others},
  eprint={2007.14062},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  year={2020}
}

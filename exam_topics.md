# Exam Topics for NLP (12 ETCs)

## A. Classical NLP
- A1. Linguistic structure and grammars (Linguistic structure, representation levels, grammars, parsing task, generation task, relation to NLP pipelines)
- A2. Elements and tasks in the traditional NLP pipeline (Structure/order of the pipeline, tokenization, sentence splitting, morphology, POS tagging, syntactic parsing, NER, coreference resolution, entity linking, WSD, semantic role labeling, semantic parsing)
- A3. Classical (whole-word) tokenization (Tokenization task definition, whitespace splitting, regular expressions, and regex cascades, lexers)
- A4. Edit distance and subword tokenization (Edit distance, subword tokenization, Byte Pair Encoding, WordPiece, SentencePiece)
- A5. Language modeling in general (Language model, continuation probabilities, role of start and end symbols, text generation, LM evaluation)
- A6. N-gram-based language modeling (Estimating sequence and word probabilities, N-gram models, markov models, Smoothing)
- A7. Text classification with classical methods (Classification tasks, bag of words, TF-IDF, naive Bayes, discriminative methods)
- A8. Sequence tagging with classical methods (Sequence tagging tasks, IOB tagging, supervised methods, HMM, Viterbi algorithm, MEMM, CRF, optimization and inference, generative and discriminative models)
- A9. Dependency parsing (Dependency grammar, projectivity, transition-based parser, graph-based parsers, non-projective parsing)
- A10. Lexical semantics based on lexical resources and LSA (Word senses and dictionaries, lexical relations, word vectors, latent semantic analysis)
- A11. Word2vec and GloVe (CBOW and Skipgram tasks, neural embeddings, training architectures, negative sampling, GloVe algorithm)
- A12. Evaluating word embeddings and embeddings based on internal word structure (Intrinsic evaluations, extrinsic evaluations, subword [fastText] embeddings)
- A13. Language modeling and sequence processing with RNNs (Four types of sequence processing, sequence tagging, bidirectional RNNs, sequence encoding, sequence generation, seq2seq tasks, LSTM architecture)

## B. LLMs in NLP
- B1. Attention mechanisms (Seq2seq basics, bottleneck problem, attention in RNN networks, properties of attention)
- B2. Attention as a layer and the transformer architecture (Dot-product attention, the role of scaling, MHA, self-attention, cross-attention)
- B3. Contextual embeddings with RNNs and transformers (Transformer-architecture, positional encoding, masking, inference and training, BERT, GPT training goals, ELMo)
- B4. Dialog systems (Types of dialog systems, general conversational requirements, open dialog systems, task-oriented dialog systems, dialog state systems, dialog state system components and implementations with LMs, simplified task-oriented dialog systems, schema-guided systems, evaluation of dialog systems)
- B5. LLM alignments (Role of alignment in AI, instruction following models, synthetic instruction datasets, supervised finetune, reinforcement learning with human feedback, chat models)
- B6. Prompt and answer engineering (Basics of LLM prompting, prompt mining and paraphrasing, gradient-based prompt optimization, prompt generation models, prefix-tuning, answer engineering, prompt ensembling, reasoning-structure-based prompting)
- B7. Embedding models and Vector-search (Role of vector-similarity search in augmented LMs, approximate nearest neighbor search, locality sensitive hashing, [product] quantization, KD-trees + priority search, graph indices, embedding-models)
- B8. Retrieval and tool-augmented LLMs (Augmented LMs in general, retrieval augmented generation, hypothetical document embedding, RAG finetuned models, self-monologue models, possibilities for tool-finetuning)
- B9. Efficient attention mechanisms (Sparse attention, factorization, positional embedding types, ALiBi, RoPe, positional interpolation, flash attention)
- B10. Distillation and quantization of LLMs (Distillation setup and training goals, weight quantization algorithms, effect of model size increase to quantization errors)
- B11. Parameter-efficient fine-tuning methods (Advantages of efficient adaptation, adapters, bottleneck adapters, low-rank adaptation, P*-tuning, intrinsic dimension and its relationship with model size)
- B12. Datasets and bootstrapping (Types of datasets needed for LM training steps, characteristics of different data sources [web, artistic, professional, etc], using LLMs for bootstrapping training)

## C. Multimodal Foundation Models
- C1. Theory of self-supervised learning and text-only contrastive models (Energy-based modeling, contrastive learning, distance-based loss functions, NCE, InfoNCE, examples of negative sampling, label supervised CL, invariant and equivariant traits in CL, contrastive learning in text-only models)
- C2. Multimodal contrastive learning and decoding methods (CLIP, emergent modality connections in multi-modal systems, iterative decoding, prefix decoding, zero-shot decoding, contrastive captioners)
- C3. Visual tokenization and variational autoencoders (Patching, continuous patch embedding methods, variational theorem [meaning, related loss function, relationship with MLE], variational autoencoder, dVAE, VQ-VAE)
- C4. Vision transformers and encoder-decoder visual-language models (Encoder-style visual transformers [ViT], using ViTs with different input resolutions, patch merging, windowed attention. Full-stack [encoder-decoder] visual-language models [e.g. TrOCR], Encoder-Prefix-decoder models [e.g. LLaVa])
- C5. Text-to-image methods (T2I GANs, T2I VAEs, goal of diffusion, DDPM [graphical model, ELBO, simplified loss, disadvantages, interpolation], DDIM [graphical model, relationship with DDPM, role of x0 prediction, simplified loss, accelerated generation])
- C6.  Applications and extensions of latent diffusion models (Classifier guidance, classifier-free guidance, latent diffusion models [encoding to latent space, diffusion process, methods of conditioning, decoding from latent space], multi-stage networks, conditional upscaling, inpainting, adaptation methods [textual inversions, adapters, ControlNets, adapters for control], latent consistency modeling)
- C7. Mixtures of experts (Properties of MoEs, modern MoE architectures in deep learning, MoE-based language models, visual-language models with modality experts, MoE adapters, Fast Feed Forward layers)
- C8. Visual-Language-Action models (Visual-Language-Navigation, Visual-Language-Action models, connections of VLA models to VL models, datasets needed for VLN and VLA tasks)
- C9. Speech to text processing (STT task definition, speech signal processing [sampling, Fourier transform, Mel spectrum], acoustic modeling, combining language models with acoustic models, connectionist temporal classification)
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Hybrid Knowledge Graph + vector database Retrieved Augmented Generation\n",
        "\n",
        "The goal is to build a knowledge graph + vector database supported Retrieved Augmented Generation, where information from a PDF file processed, these information is stored in the databases. When a user query comes retrieve context information which helps the answer generation of the selected model.\n",
        "\n",
        "### Overview:\n",
        "\n",
        "1. Install required packages\n",
        "2. Initialize the two databases\n",
        "3. Split the text into sematically relevant chunks\n",
        "4. Build the knowledge graph and vector database\n",
        "  - Extract **subject** ---**predicate**---> **object** triplets from each chunks\n",
        "  - Refining triplets, actualizing triplet information, merge information of same entities\n",
        "6. Write retrieving process\n",
        "7. Ask the model questions regarding the PDF, compare the results while using no context"
      ],
      "metadata": {
        "id": "46sgpDamhk87"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install all required packages"
      ],
      "metadata": {
        "id": "cX4m8k5Ak04P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index==0.11.13 -q\n",
        "!pip install llama-index-llms-openai==0.2.9 -q\n",
        "!pip install llama-index-embeddings-openai==0.2.5 -q\n",
        "!pip install llama-index-readers-file==0.2.2 -q\n",
        "!pip install nebula3-python==3.8.2 -q\n",
        "!pip install nebulagraph-lite==0.2.5 -q\n",
        "!pip install ipython-ngql==0.14.3 -q\n",
        "!pip install pyvis==0.3.2 -q\n",
        "!pip install ipython==7.34.0 -q\n",
        "!pip install chromadb==0.5.7 -q\n",
        "!pip install PyPDF2==3.0.1 -q\n",
        "!pip install nltk==3.9.1 -q"
      ],
      "metadata": {
        "id": "IBmXoJ92WU3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###"
      ],
      "metadata": {
        "id": "MYa_d_L9k_Ef"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import everything which is necessary"
      ],
      "metadata": {
        "id": "2PbhPAZklGQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tiktoken\n",
        "from google.colab import userdata\n",
        "\n",
        "from llama_index.core import Settings\n",
        "from llama_index.core.prompts import ChatMessage, PromptTemplate, ChatPromptTemplate\n",
        "from llama_index.core.base.llms.types import (\n",
        "    ChatMessage,\n",
        "    MessageRole,\n",
        ")\n",
        "\n",
        "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "\n",
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
        "from llama_index.readers.file import PDFReader\n",
        "from llama_index.core.memory import ChatMemoryBuffer\n",
        "\n",
        "from pyvis.network import Network\n",
        "from pyvis.options import Layout\n",
        "from IPython.display import IFrame, HTML\n",
        "\n",
        "import chromadb\n",
        "from chromadb.utils import embedding_functions\n",
        "\n",
        "from random import shuffle\n",
        "import nltk\n",
        "import json\n",
        "\n",
        "import requests\n",
        "from PyPDF2 import PdfReader, PdfWriter\n",
        "from io import BytesIO\n",
        "import httpx\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "8aE-FVSenkiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initializing NebulaGraph Database, more info about the database can be found on [NebulaGraph](https://docs.nebula-graph.io/3.8.0/)\n",
        "The documentation for the NGQL query language used by NebulaGraph can be found on [NebulaGraph Docs](https://docs.nebula-graph.io/3.0.0/3.ngql-guide/1.nGQL-overview/1.overview/).\n",
        "\n",
        "You can find relationship related queries in the **Graph Patterns** section of the documentation"
      ],
      "metadata": {
        "id": "zb_ljomxlRPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nebulagraph_lite import nebulagraph_let as ng_let\n",
        "\n",
        "n = ng_let()\n",
        "\n",
        "# This takes around 5 mins\n",
        "n.start()"
      ],
      "metadata": {
        "id": "ODYJkBTXlliv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext ngql\n",
        "%config IPythonNGQL.ngql_result_style=\"raw\"\n"
      ],
      "metadata": {
        "id": "2PGIm_I4lu7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%ngql --address 127.0.0.1 --port 9669 --user root --password nebula"
      ],
      "metadata": {
        "id": "-sI5AD3hl3Ug",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fac639e2-510e-4d0b-e135-9adb5985e22f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;38;2;0;135;107m[OK] Connection Pool Created\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResultSet(keys: ['Name'], values: [\"basketballplayer\"])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NebulaHandler:\n",
        "    def __init__(self, space_name):\n",
        "        \"\"\"\n",
        "          Initializes the NebulaHandler object and specify space name.\n",
        "\n",
        "          Args:\n",
        "              space_name (str): The name of the space to initialize and switch to.\n",
        "        \"\"\"\n",
        "        self.space_name = space_name\n",
        "\n",
        "    def switch_space(self, space_name):\n",
        "        \"\"\"\n",
        "          Switches the space in the NebulaGraph database.\n",
        "\n",
        "          Args:\n",
        "              space_name (str): The name of the space to switch to.\n",
        "        \"\"\"\n",
        "        self.space_name = space_name\n",
        "\n",
        "    def recreate_space(self):\n",
        "        \"\"\"\n",
        "          Recreates the specified space in the NebulaGraph database.\n",
        "          This includes dropping:\n",
        "          - the existing space\n",
        "          - creating a new space,\n",
        "          - defining the entity and relationship structure.\n",
        "        \"\"\"\n",
        "        query = f'''\n",
        "            DROP SPACE IF EXISTS {self.space_name};\n",
        "            CREATE SPACE {self.space_name}(partition_num=1, replica_factor=1, vid_type=INT64);\n",
        "            USE {self.space_name};\n",
        "            CREATE TAG entity(name string, description string);\n",
        "            CREATE EDGE relationship(relationship string);\n",
        "            CREATE TAG INDEX entity_name_index ON entity();\n",
        "            CREATE EDGE INDEX relationship_index ON relationship();\n",
        "        '''\n",
        "        %ngql {query}\n",
        "\n",
        "\n",
        "    def get_max_entity_idx(self):\n",
        "        \"\"\"\n",
        "          Retrieves the maximum entity ID in the current space, used for assigning new entity IDs.\n",
        "\n",
        "          Returns:\n",
        "              int: The highest entity ID or 0 if there are no entities.\n",
        "        \"\"\"\n",
        "        query = f'''\n",
        "            USE {self.space_name};\n",
        "            MATCH (v:entity) RETURN id(v) AS id ORDER BY id DESC LIMIT 1\n",
        "        '''\n",
        "        result = %ngql {query}\n",
        "        entity_idx = result.column_values('id')\n",
        "        if len(entity_idx)>0:\n",
        "            entity_idx = entity_idx[0].as_int()\n",
        "        else:\n",
        "            entity_idx = 0\n",
        "\n",
        "        return entity_idx\n",
        "\n",
        "    def insert_entity(self, entity_name, entity_description, entity_idx = None):\n",
        "        \"\"\"\n",
        "          Inserts a new entity with the given name and description into the space.\n",
        "          If no entity_idx is provided, a new one is generated.\n",
        "\n",
        "          Args:\n",
        "              entity_name (str): The name of the entity.\n",
        "              entity_description (str): A description of the entity.\n",
        "              entity_idx (int, optional): The ID to assign to the entity. If not provided, auto-generated.\n",
        "\n",
        "          Returns:\n",
        "              int: The ID of the inserted entity.\n",
        "        \"\"\"\n",
        "        if entity_idx is None:\n",
        "            entity_idx = str(self.get_max_entity_idx()+1)\n",
        "\n",
        "        entity_name = entity_name.replace(\"'\", \"\\\\'\")\n",
        "        entity_description = entity_description.replace(\"'\", \"\\\\'\")\n",
        "\n",
        "        query = f'''\n",
        "            USE {self.space_name};\n",
        "            INSERT VERTEX entity(name, description) VALUES {entity_idx}:(\"{entity_name}\", \"{entity_description}\");\n",
        "        '''\n",
        "        resp = %ngql {query}\n",
        "\n",
        "            #print(entity_idx, entity_name)\n",
        "        # print(resp.__dict__)\n",
        "\n",
        "        if not resp._resp.error_code:\n",
        "          print(\"Insert operation successful!\")\n",
        "        else:\n",
        "          print(\"Error occurred during insert:\", resp)\n",
        "\n",
        "        return entity_idx\n",
        "\n",
        "    def insert_relationship(self, src_entity_idx, dst_entity_idx, relationship):\n",
        "        \"\"\"\n",
        "          Inserts a relationship (edge) between two entities in the space.\n",
        "\n",
        "          Args:\n",
        "              src_entity_idx (int): The ID of the source entity.\n",
        "              dst_entity_idx (int): The ID of the destination entity.\n",
        "              relationship (str): The relationship description between the two entities.\n",
        "        \"\"\"\n",
        "        query = f'''\n",
        "              USE {self.space_name};\n",
        "              INSERT EDGE relationship(relationship) VALUES {src_entity_idx}->{dst_entity_idx}:('{relationship}')\n",
        "        '''\n",
        "        relationship = relationship.replace(\"'\", \"\\\\'\")\n",
        "            #print(src_entity_idx, dst_entity_idx, relationship)\n",
        "            #print(resp.__dict__)\n",
        "        resp = %ngql {query}\n",
        "        if not resp._resp.error_code:\n",
        "          print(\"Insert operation successful!\")\n",
        "        else:\n",
        "          print(\"Error occurred during insert:\", resp)\n",
        "\n",
        "    def upsert_entity_relationship(self, src_name, src_description, dst_name, dst_description, relationship):\n",
        "        \"\"\"\n",
        "          Upserts (inserts or updates) two entities and their relationship in the space.\n",
        "\n",
        "          Args:\n",
        "              src_name (str): The name of the source entity.\n",
        "              src_description (str): The description of the source entity.\n",
        "              dst_name (str): The name of the destination entity.\n",
        "              dst_description (str): The description of the destination entity.\n",
        "              relationship (str): The relationship description between the two entities.\n",
        "\n",
        "          Returns:\n",
        "              tuple: A tuple containing the IDs of the source and destination entities.\n",
        "        \"\"\"\n",
        "        src_entity_idx = self.get_id_by_name(src_name)\n",
        "        dst_entity_idx = self.get_id_by_name(dst_name)\n",
        "        src_entity_idx = self.insert_entity(src_name, src_description, src_entity_idx)\n",
        "        dst_entity_idx = self.insert_entity(dst_name, dst_description, dst_entity_idx)\n",
        "        self.insert_relationship(src_entity_idx, dst_entity_idx, relationship)\n",
        "        return src_entity_idx, dst_entity_idx\n",
        "\n",
        "\n",
        "    def get_id_by_name(self, entity_name):\n",
        "        \"\"\"\n",
        "          Retrieves the ID of an entity by its name.\n",
        "\n",
        "          Args:\n",
        "              entity_name (str): The name of the entity.\n",
        "\n",
        "          Returns:\n",
        "              int or None: The ID of the entity, or None if the entity does not exist.\n",
        "        \"\"\"\n",
        "        query = f'''\n",
        "            USE {self.space_name};\n",
        "            MATCH (v:entity) WHERE v.entity.name == \"{entity_name}\" RETURN id(v) AS id\n",
        "        '''\n",
        "        entity_name = entity_name.replace(\"'\", \"\\\\'\")\n",
        "        resp = %ngql {query}\n",
        "        if len(resp.column_values(\"id\"))>0:\n",
        "            entity_idx = str(resp.column_values(\"id\")[0].as_int())\n",
        "        else:\n",
        "            entity_idx = None\n",
        "\n",
        "        print(entity_name, \"found under idx\", entity_idx)\n",
        "        return entity_idx\n",
        "\n",
        "    def get_data_by_id(self, entity_idx):\n",
        "        \"\"\"\n",
        "          Retrieves all data for an entity by its ID.\n",
        "\n",
        "          Args:\n",
        "              entity_idx (int): The ID of the entity.\n",
        "\n",
        "          Returns:\n",
        "              dict: A dictionary of entity data.\n",
        "        \"\"\"\n",
        "        query = f'''\n",
        "            USE {self.space_name};\n",
        "            MATCH (v:entity) WHERE id(v) == {entity_idx} RETURN v\n",
        "        '''\n",
        "\n",
        "        resp = %ngql {query}\n",
        "        entity_data = resp.dict_for_vis()[\"nodes_dict\"]\n",
        "        return entity_data\n",
        "\n",
        "    def get_all_data_in_hops(self, entity_name_list, hops):\n",
        "        \"\"\"\n",
        "          Retrieves all entities and relationships within a certain number of hops from the given entity names.\n",
        "\n",
        "          Args:\n",
        "              entity_name_list (list): A list of entity names to search from.\n",
        "              hops (int): The number of hops to traverse relationships.\n",
        "\n",
        "          Returns:\n",
        "              tuple: A tuple containing the nodes and edges dictionaries.\n",
        "        \"\"\"\n",
        "        # print(\"Getting data for\")\n",
        "        # print(\"entity_name_list\", entity_name_list)\n",
        "\n",
        "        query = f'''\n",
        "            USE {self.space_name};\n",
        "            MATCH p=(v:entity)-[r*1..{hops}]-(v1:entity) WHERE v.entity.name IN {entity_name_list} RETURN p\n",
        "        '''\n",
        "        entity_name_list = [name.replace(\"'\", \"\\\\'\") for name in entity_name_list]\n",
        "        resp = %ngql {query}\n",
        "        # print(resp.__dict__)\n",
        "        data = resp.dict_for_vis()\n",
        "        #print(data)\n",
        "\n",
        "        return data[\"nodes_dict\"], data[\"edges_dict\"]\n",
        "\n",
        "    def get_all_node_names(self):\n",
        "        \"\"\"\n",
        "          Retrieves the names of all nodes (entities) in the space.\n",
        "\n",
        "          Returns:\n",
        "              list: A list of all node names.\n",
        "        \"\"\"\n",
        "        query = f'''\n",
        "            USE {self.space_name};\n",
        "            MATCH (v:entity) RETURN v.entity.name\n",
        "        '''\n",
        "\n",
        "        resp = %ngql {query}\n",
        "        node_names = resp.column_values(\"v.entity.name\")\n",
        "\n",
        "        if len(node_names) == 0:\n",
        "            names = []\n",
        "        else:\n",
        "            try:\n",
        "                names = [n.as_string() for n in node_names]\n",
        "            except:\n",
        "                names = []\n",
        "\n",
        "        #print(names)\n",
        "        return names\n",
        "\n",
        "\n",
        "    def get_description_by_name(self, entity_name):\n",
        "        \"\"\"\n",
        "          Retrieves the description of an entity by its name.\n",
        "\n",
        "          Args:\n",
        "              entity_name (str): The name of the entity.\n",
        "\n",
        "          Returns:\n",
        "              str or None: The description of the entity, or None if not found.\n",
        "        \"\"\"\n",
        "        query = f'''\n",
        "            USE {self.space_name};\n",
        "            MATCH (v:entity) WHERE v.entity.name == \"{entity_name}\" RETURN v.entity.description\n",
        "        '''\n",
        "        entity_name = entity_name.replace(\"'\", \"\\\\'\")\n",
        "        resp = %ngql {query}\n",
        "        #print(\"getting\",entity_name, \"description\\n\",resp.__dict__)\n",
        "        entity_description = resp.column_values(\"v.entity.description\")\n",
        "        if len(entity_description) > 0:\n",
        "            entity_description = entity_description[0].as_string()\n",
        "        else:\n",
        "            entity_description = None\n",
        "\n",
        "        return entity_description\n",
        "\n",
        "    def get_full_graph(self):\n",
        "        \"\"\"\n",
        "          Retrieves the full graph of all entities and relationships in the space.\n",
        "\n",
        "          Returns:\n",
        "              tuple: A tuple containing the nodes and edges dictionaries.\n",
        "        \"\"\"\n",
        "        query = f'''\n",
        "            USE {self.space_name};\n",
        "            MATCH p=(v:entity)-[r]->(v1:entity) RETURN p\n",
        "        '''\n",
        "\n",
        "        resp = %ngql {query}\n",
        "        data = resp.dict_for_vis()\n",
        "\n",
        "        return data[\"nodes_dict\"], data[\"edges_dict\"]"
      ],
      "metadata": {
        "id": "6ka7dn0el2Qj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initializing Vector Database\n",
        "Because of the limitations of Google Colab we use [ChromaDB](https://www.trychroma.com/). But for this purpose, [Weaviate](https://weaviate.io/) is a better alternative if you are using it locally or from the cloud."
      ],
      "metadata": {
        "id": "YMUTQO4-l99a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ChromaHandler:\n",
        "\n",
        "    def __init__(self, collection_name):\n",
        "        \"\"\"\n",
        "        Initializes the ChromaHandler object and switches to the specified collection.\n",
        "\n",
        "        Args:\n",
        "            collection_name (str): The name of the collection to initialize and switch to.\n",
        "        \"\"\"\n",
        "        self.collection_name = collection_name\n",
        "        self.switch_collection(collection_name)\n",
        "\n",
        "    def embedding_function(self):\n",
        "        \"\"\"\n",
        "        Returns the embedding function for Chroma.\n",
        "        Here OpenAI embeddings used, could be changed to other embeddings.\n",
        "\n",
        "        Returns:\n",
        "            embedding_functions.OpenAIEmbeddingFunction: The embedding function for Chroma.\n",
        "        \"\"\"\n",
        "        openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
        "                api_key=API_TOKEN,\n",
        "                model_name=OPENAI_EMBEDDING_MODEL_NAME,\n",
        "            )\n",
        "        return openai_ef\n",
        "\n",
        "    def get_all_collections_name(self):\n",
        "        \"\"\"\n",
        "        Retrieves the names of all collections in the Chroma database.\n",
        "\n",
        "        Returns:\n",
        "            list: A list of collection names.\n",
        "        \"\"\"\n",
        "        collections = chromadb.Client().list_collections()\n",
        "        collections = [c.name for c in collections]\n",
        "        return collections\n",
        "\n",
        "\n",
        "    def switch_collection(self, collection_name):\n",
        "        \"\"\"\n",
        "        Switches the collection in the Chroma database.\n",
        "\n",
        "        Args:\n",
        "            collection_name (str): The name of the collection to switch to.\n",
        "        \"\"\"\n",
        "        self.collection_name = collection_name\n",
        "        client = chromadb.Client()\n",
        "\n",
        "        if self.collection_name in self.get_all_collections_name():\n",
        "            self.collection = client.get_collection(collection_name)\n",
        "        else:\n",
        "            self.recreate_collection(collection_name)\n",
        "\n",
        "    def recreate_collection(self, collection_name):\n",
        "        \"\"\"\n",
        "        Recreates the specified collection in the Chroma database.\n",
        "\n",
        "        Args:\n",
        "            collection_name (str): The name of the collection to recreate.\n",
        "        \"\"\"\n",
        "        client = chromadb.Client()\n",
        "        if self.collection_name in self.get_all_collections_name():\n",
        "            client.delete_collection(collection_name)\n",
        "        client.create_collection(collection_name, embedding_function=self.embedding_function())\n",
        "\n",
        "\n",
        "    def upsert_entity(self, entity_name, entity_description, vid, text):\n",
        "      \"\"\"\n",
        "        Upserts (inserts or updates) an entity into the specified collection with its metadata.\n",
        "\n",
        "        Args:\n",
        "            entity_name (str): The name of the entity.\n",
        "            entity_description (str): A description of the entity to be stored.\n",
        "            vid (str): The unique ID of the entity.\n",
        "            text (str): Additional text metadata related to the entity.\n",
        "        \"\"\"\n",
        "      client = chromadb.Client()\n",
        "      collection = client.get_collection(self.collection_name)\n",
        "      collection.upsert(\n",
        "              documents=[entity_description],\n",
        "              metadatas=[{\"name\": entity_name, \"texts\": text}],\n",
        "              ids=[vid]\n",
        "          )\n",
        "      result = collection.get(where={\"name\": entity_name})\n",
        "\n",
        "      # print(result)\n",
        "\n",
        "\n",
        "    def search_entity_by_description(self, query, top_k=5):\n",
        "        \"\"\"\n",
        "        Searches for entities in the collection by their description and retrieves metadata.\n",
        "\n",
        "        Args:\n",
        "            query (str): The query string to search for in the entity descriptions.\n",
        "            top_k (int, optional): The maximum number of results to return. Defaults to 5.\n",
        "\n",
        "        Returns:\n",
        "            List[dict]: A list of objects, each containing entity details (vid, name, description, texts).\n",
        "        \"\"\"\n",
        "        client = chromadb.Client()\n",
        "        collection = client.get_collection(self.collection_name)\n",
        "        results = collection.query(\n",
        "            query_texts=[query],\n",
        "            n_results=top_k,\n",
        "            include=[\"documents\", \"metadatas\"])\n",
        "\n",
        "        objects = []\n",
        "\n",
        "        for index, _ in enumerate(results[\"documents\"][0]):\n",
        "            data = {\"vid\": results['ids'][0][index], \"name\": results[\"metadatas\"][0][index][\"name\"], \"description\": results[\"documents\"][0][index], \"texts\": results[\"metadatas\"][0][index][\"texts\"]}\n",
        "            objects.append(data)\n",
        "        return objects"
      ],
      "metadata": {
        "id": "yRf6SAzcmhT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get the document we will use. For this example we used the writing of Leopold Aschenbrenner titled [**Situational Awareness**](https://situational-awareness.ai/) and get a 30 page chunk from it.\n",
        "\n",
        "You will see that this book or wiriting is about the future, how the models will develop, and how these models will perform. These concepts are theoretical and these are the guesses of the author for the future."
      ],
      "metadata": {
        "id": "rGmjVAsunAP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_and_extract_pdf(url, output_filename, start_page=46, page_count=30):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        pdf_data = BytesIO(response.content)\n",
        "        reader = PdfReader(pdf_data)\n",
        "\n",
        "        writer = PdfWriter()\n",
        "\n",
        "        total_pages = len(reader.pages)\n",
        "\n",
        "        start_index = start_page - 1\n",
        "        end_index = min(start_index + page_count, total_pages)\n",
        "\n",
        "        for page_num in range(start_index, end_index):\n",
        "            writer.add_page(reader.pages[page_num])\n",
        "\n",
        "        with open(output_filename, \"wb\") as output_pdf:\n",
        "            writer.write(output_pdf)\n",
        "\n",
        "        print(f\"Extracted pages {start_page}-{end_index} from the PDF and saved to {output_filename}.\")\n",
        "    else:\n",
        "        print(f\"Failed to download the PDF. Status code: {response.status_code}\")\n",
        "\n",
        "# Here you can add your own PDF URL if zou want to try out other documents\n",
        "pdf_url = \"https://situational-awareness.ai/wp-content/uploads/2024/06/situationalawareness.pdf\"\n",
        "output_pdf = \"KG_RAG.pdf\"\n",
        "download_and_extract_pdf(pdf_url, output_pdf)"
      ],
      "metadata": {
        "id": "v9zdMYyZm-p8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f239d441-7e89-4dc4-f0ed-c973df661458"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted pages 46-75 from the PDF and saved to KG_RAG.pdf.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize the models we will use\n",
        "\n",
        "- ChatModel: GPT4-o-mini, this is a cost-effective version of GPT-4o\n",
        "- Embedding Model: text-embedding-3-small from OpenAI\n",
        "- Tokenizer model: using GPT-4o with tiktoken library which is a fast BPE tokenizer using OpenAI models\n",
        "\n",
        "You will need an API Token From the OpenAI API website, and also around 5 dollars uploaded to your account ([Guide for making an OpenAI API account](https://help.noteplan.co/article/207-set-personal-openai-api-key))\n",
        "\n",
        "I used the Google Colab **Secret** feature to store API token and to use it in code. ([Here is a tutorial how to use Colab Secrets](https://medium.com/@parthdasawant/how-to-use-secrets-in-google-colab-450c38e3ec75))\n",
        "\n",
        "You must set the ***OPENAI_API_KEY*** environment variable to you API TOKEN!!!!!\n"
      ],
      "metadata": {
        "id": "duSm6cbPpNF4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### If you are not running this on colab, please set the ***OPENAI_API_KEY*** to you API KEY"
      ],
      "metadata": {
        "id": "MSA5KAwMTib9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "API_TOKEN = userdata.get('OPEN_AI_KEY')\n",
        "os.environ['OPENAI_API_KEY'] = API_TOKEN"
      ],
      "metadata": {
        "id": "UEEu1J-Fp66I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OPENAI_CHAT_MODEL_NAME=\"gpt-4o-mini\" # Change this to use a different model\n",
        "OPENAI_EMBEDDING_MODEL_NAME=\"text-embedding-3-small\""
      ],
      "metadata": {
        "id": "0B2DtYeWpL0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_llms():\n",
        "    Settings.llm = OpenAI(temperature=0.3, model=OPENAI_CHAT_MODEL_NAME)\n",
        "    Settings.embed_model = OpenAIEmbedding(model_name=OPENAI_EMBEDDING_MODEL_NAME, mode=\"similarity\")\n",
        "    Settings.tokenizer = tiktoken.encoding_for_model(OPENAI_CHAT_MODEL_NAME)\n",
        "\n",
        "init_llms()"
      ],
      "metadata": {
        "id": "bEZ-vdeDp9pV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's destroy our PDF into semantically relevant chunks\n",
        "\n",
        "The sentences of the PDF are split and embedded in groups of 3. Then the embedding changes (that capture the semantics, the meaning of the given sentence group) are computed over all consecutive sentence groups. The highest changes (upper 95 percentile) are selected as chunk boundaries. This is done to avoid splitting corresponding sentences into different chunks.\n",
        "This chunking method is somewhat crude as layout is ignored and sentences are split using regex only.\n",
        "For more details see: https://www.youtube.com/watch?v=8OJC21T2SL4&t=1933s"
      ],
      "metadata": {
        "id": "aCcfes4mo2Jn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_pdf(document_path):\n",
        "    \"\"\"\n",
        "    Parses a PDF document and returns a list of nodes extracted from the document.\n",
        "\n",
        "    Args:\n",
        "        document_path (str): The path to the PDF document.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of nodes extracted from the document.\n",
        "    \"\"\"\n",
        "    documents = PDFReader().load_data(document_path)\n",
        "\n",
        "    splitter = SemanticSplitterNodeParser(\n",
        "        buffer_size=5, breakpoint_percentile_threshold=95, embed_model=Settings.embed_model\n",
        "    )\n",
        "    nodes = splitter.get_nodes_from_documents(documents)\n",
        "    for node in nodes:\n",
        "        node.text = node.text.replace(\"“\", '\"').replace(\"”\", '\"').replace(\"‘\", \"'\").replace(\"’\", \"'\")\n",
        "    return nodes"
      ],
      "metadata": {
        "id": "gV41T7gvo1WN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the spce_name used in the Databases\n",
        "space_name = \"KG_RAG\""
      ],
      "metadata": {
        "id": "AfsoKOM0q5e_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "docs = parse_pdf(output_pdf)\n",
        "\n",
        "print(f\"Extracted {len(docs)} chunks from {output_pdf}\")\n",
        "vec_db = ChromaHandler(space_name)\n",
        "graph_db = NebulaHandler(space_name)\n",
        "\n",
        "vec_db.switch_collection(space_name)\n",
        "graph_db.switch_space(space_name)\n",
        "\n",
        "vec_db.recreate_collection(space_name)\n",
        "graph_db.recreate_space()"
      ],
      "metadata": {
        "id": "4sCx4Ajxq2_X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa1e7551-24a7-4d85-b2eb-8360cdcd019f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted 60 chunks from KG_RAG.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining prompt templates.\n",
        "\n",
        "With GPT-4o-mini JSON was used, to make the communication more structured. (You will see the resaon below). It is good practice to use only JSON for communcation (JSON goes in, JSON goes out) because if we want structured data as output, JSON object is easy-to-process from code.\n",
        "\n",
        "For using JSON with a model shcemas can be added to the model, but from experience, an excample JSON object what we want as output works better tha using shcemas.\n",
        "\n",
        "Here OpenAI presents the Structured output case (using schema) and JSON mode case [Schema and JSON mode](https://platform.openai.com/docs/guides/structured-outputs/how-to-use?context=ex1)\n",
        "\n",
        "\n",
        "We add the instructions and the JSON example in the system prompt, and only providing a JSON as a user prompt."
      ],
      "metadata": {
        "id": "V8ISnEtm-Hf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "extract_triplets_chat_templ = ChatPromptTemplate(message_templates=[\n",
        "    ChatMessage(content=\"\"\"Given a text chunk from a document extract up to {max_knowledge_triplets} and not more knowledge triplets in the form of (subject, predicate, object).\n",
        "Stopwords should be avoided. Provide the triples as a JSON object along with a short description of the subject and object entities. Descriptions could be longer than the example provided.\n",
        "Use English regardless of the input data.\n",
        "\n",
        "Example:\n",
        "Text: Philz is a coffee shop founded in 1982 in Berkeley. The café specializes in handcrafted coffee.\n",
        "\n",
        "Response example:\n",
        "{{\n",
        "   \"triplets\": [\n",
        "    {{\n",
        "        \"subject\": {{\"name\": \"Philz\", \"description\": \"The name of a coffee shop\"}},\n",
        "        \"predicate\": \"type of business\",\n",
        "        \"object\": {{\"name\": \"coffee shop\", \"description\": \"A place specializing in serving coffee\"}}\n",
        "    }},\n",
        "    {{\n",
        "        \"subject\": {{\"name\": \"Philz\", \"description\": \"The name of a coffee shop\"}},\n",
        "        \"predicate\": \"located in\",\n",
        "        \"object\": {{\"name\": \"Berkeley\", \"description\": \"A city in California\"}}\n",
        "    }},\n",
        "    {{\n",
        "        \"subject\": {{\"name\": \"Philz\", \"description\": \"The name of a coffee shop\"}},\n",
        "        \"predicate\": \"founded in\",\n",
        "        \"object\": {{\"name\": \"1982\", \"description\": \"A year, date\"}}\n",
        "    }},\n",
        "    {{\n",
        "        \"subject\": {{\"name\": \"Philz\", \"description\": \"The name of a coffee shop\"}},\n",
        "        \"predicate\": \"specializes in\",\n",
        "        \"object\": {{\"name\": \"handcrafted coffee\", \"description\": \"Coffee made by hand, generally of high quality\"}}\n",
        "    }}\n",
        "  ]\n",
        "}}\"\"\", role=\"system\"),\n",
        "    ChatMessage(content=\"{text}\", role=\"user\")\n",
        "]\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "mSy446pYTl3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "refine_triplet_chat_templ = ChatPromptTemplate(message_templates=[\n",
        "    ChatMessage(content=\"\"\"Your task is to decide if the knowledge triplet extracted from a document could be refined by finding similar nodes in the graph. Use a proper JSON format!\n",
        "    Use English regardless of the input data.\n",
        "Example query:\n",
        "{{\n",
        "    \"document\": \"Philz is a coffee shop founded in 1982 in Berkeley. The café specializes in handcrafted coffee.\",\n",
        "    \"triplet\": {{\n",
        "        \"subject\": \"Philz\",\n",
        "        \"predicate\": \"type of business\",\n",
        "        \"object\": \"coffee shop\"\n",
        "    }},\n",
        "    \"new_nodes\": [\n",
        "        {{\n",
        "            \"entity\": \"Philz\",\n",
        "            \"description\": \"The name of a coffee shop.\"\n",
        "        }},\n",
        "        {{\n",
        "            \"entity\": \"coffee shop\",\n",
        "            \"description\": \"A place specializing in serving coffee.\"\n",
        "        }}\n",
        "    ],\n",
        "    \"existing_nodes\": [\n",
        "        {{\n",
        "            \"entity\": \"Philz Coffee Shop\",\n",
        "            \"description\": \"A coffee shop located in Berkeley.\"\n",
        "        }},\n",
        "        {{\n",
        "            \"entity\": \"Berkeley\",\n",
        "            \"description\": \"A city in California.\"\n",
        "        }},\n",
        "        {{\n",
        "            \"entity\": \"1982\",\n",
        "            \"description\": \"A year.\"\n",
        "        }}\n",
        "    ]\n",
        "}}\n",
        "Example response:\n",
        "{{\n",
        "    \"reasoning\": \"'Philz' is likely a shorthand for 'Philz Coffee Shop.' The coffee shop node doesn't yet exist in the graph in a general sense.\",\n",
        "    \"refined_triplet\": {{\n",
        "        \"subject\": {{\n",
        "            \"name\": \"Philz Coffee Shop\",\n",
        "            \"description\": \"A coffee shop located in Berkeley, often referred to as 'Philz.'\"\n",
        "        }},\n",
        "        \"predicate\": \"type of business\",\n",
        "        \"object\": {{\n",
        "            \"name\": \"coffee shop\",\n",
        "            \"description\": \"A place specializing in serving coffee.\"\n",
        "        }}\n",
        "    }}\n",
        "}}\n",
        "\n",
        "Provide a reasoning for your decision and suggest a refined triplet if necessary using the existing nodes, or keeping the new nodes if they are not present in the graph with a good enough precision.\n",
        "If you select an already existing node extend the description of that node with the new information provided in the document.\"\"\", role=\"system\"),\n",
        "ChatMessage(content=\"{datajson}\", role=\"user\")\n",
        "]\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "G_mAq4os_7kJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question_forming_chat_templ = ChatPromptTemplate(message_templates=[\n",
        "    ChatMessage(content=\"\"\"Given the user's last request extract a list of at most {max_entities} entities/keyphrases that the user is interested in.\n",
        "In a proper JSON format:\n",
        "Generate a standalone question resolving any references to the provided chat history.\n",
        "Generate a hypothetical descriptions for the entities/keyphrases.\n",
        "Generate a hypothetical answer to the question.\n",
        "Use English regardless of the input data.\n",
        "\n",
        "Example input:\n",
        "{{\"chat\": [\n",
        "  {{\"role\": \"user\", \"content\": \"Hi! I'd like to find a café in Berkeley.\"}},\n",
        "  {{\"role\": \"assistant\", \"content\": \"There are several cafés in Berkeley. What kind of coffee are you looking for?\"}},\n",
        "  {{\"role\": \"user\", \"content\": \"I'm specifically looking for original black coffee.\"}}] }}\n",
        "\n",
        "Example output:\n",
        "{{\n",
        "  \"question\": \"Where can I find a store that sells original black coffee in Berkeley?\",\n",
        "  \"phrases\": [\n",
        "      {{\"entity\": \"café\", \"description\": \"A place that specializes in serving coffee\"}},\n",
        "      {{\"entity\": \"Berkeley\", \"description\": \"A city in California\"}},\n",
        "      {{\"entity\": \"original black coffee\", \"description\": \"A type of coffee that is not mixed with other varieties and contains no added flavors.\"}}\n",
        "  ],\n",
        "  \"hypothetical_answer\": \"One of the most well-known cafés selling original black coffee is Philz Café in Berkeley.\"}}\n",
        "}}\n",
        "\n",
        "Use null for all the parameters if the user just says hi, or there is no information that you could extract from the conversation.\"\"\", role=\"system\"),\n",
        "ChatMessage(content=\"{historyjson}\", role=\"user\")\n",
        "]\n",
        ")\n",
        "\n",
        "\n",
        "bot_prefix_chat_templ = ChatPromptTemplate(message_templates=[\n",
        "    ChatMessage(content=\"\"\"You are a helpful assistant. You answer the user's question based on the context provided here.\n",
        "Do not make up data, ground your answers in the context.\n",
        "The context contains a set of document chunks, knowledge graph nodes and knowledge graph connections. Use these to answer the question.\n",
        "Do not mention this knowledge graph directly, use fluent natural language instead.\n",
        "Use English regardless of the input data.\n",
        "\n",
        "### Context ###\n",
        "{context_info}\"\"\", role=\"system\"),\n",
        "ChatMessage(content=\"{starter_message}\", role=\"assistant\")\n",
        "])\n",
        "\n",
        "bot_prefix_chat_without_context_templ = ChatPromptTemplate(message_templates=[ChatMessage(content=\"\"\"You are a helpful assistant. You answer the uses's question. Do not provide any intermediate\n",
        "step thinking or reasoning just the answer for the question\"\"\", role=\"system\"),\n",
        "    ChatMessage(content=\"{starter_message}\", role=\"assistant\")\n",
        "])\n"
      ],
      "metadata": {
        "id": "skd2xQVIOX2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining Tokencheker class\n",
        "\n",
        "This class will use a tokenizer to limit the number of input tokens to the ***max_tokens*** treshold.\n",
        "\n",
        "This is important in cases where the chun text or the prompt text is too long and does not fit the context window then need to drop the extra tokens."
      ],
      "metadata": {
        "id": "9YvqJFHIANJI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenChecker:\n",
        "    def __init__(self, model_name):\n",
        "        self.tokenizer = tiktoken.encoding_for_model(model_name)\n",
        "\n",
        "    def restrict(self, input_text, max_tokens, return_len=False):\n",
        "        tokens = self.tokenizer.encode(input_text)\n",
        "        restricted_tokens = tokens[:max_tokens]\n",
        "        restricted_text = self.tokenizer.decode(restricted_tokens)\n",
        "\n",
        "        if return_len:\n",
        "            return restricted_text, len(tokens)\n",
        "        else:\n",
        "            return restricted_text\n"
      ],
      "metadata": {
        "id": "XjF8GNTNZa2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_tokenizer = TokenChecker(OPENAI_EMBEDDING_MODEL_NAME)\n",
        "chat_tokenizer = TokenChecker(OPENAI_CHAT_MODEL_NAME)"
      ],
      "metadata": {
        "id": "Pr9O33QxkZTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining Triplet extarcting and triplet refining functions\n",
        "\n",
        "####Triplet extraction   \n",
        "The chunks are then consecutively processed to extract\n",
        "`subject-predicate-object` triplets. Each node of the triplets is paired up with a corresponding description.\n",
        "\n",
        "####Consistency check   \n",
        "Since the triplet extraction handles each node separately finding previously extracted nodes and merging them with the currently proposed triplets is necessary. Each triplet is checked for consistency with the existing graph. If a node's name is similar (2 characters away) from an existing node they are automatically considered as an alternative. This set of alternative, already existing nodes is then expanded via a similarity search in the node description embedding space (regular retrieval of the top 5 most similar nodes). Then OpenAI is prompted with the alternative node information and the new proposed triplet, after a short reasoning the LLM (used in JSON mode) decides if any of the nodes in the new triplet should be merged with an existing alternative node. If nodes are merged the description of the existing node is updated to reflect the new information. This is done to avoid duplicates and to keep the graph clean. This method is \"experimental\" as there is no prior method that achieves the same."
      ],
      "metadata": {
        "id": "tli1lPz9BW-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_triplets(num_triplets, text):\n",
        "    \"\"\"\n",
        "    Extracts knowledge triplets from a given text.\n",
        "\n",
        "    Args:\n",
        "        num_triplets (int): The number of knowledge triplets to extract.\n",
        "        text (str): The text from which to extract the knowledge triplets.\n",
        "\n",
        "    Returns:\n",
        "        str: The extracted knowledge triplets.\n",
        "    \"\"\"\n",
        "\n",
        "    text = embedding_tokenizer.restrict(text, 120000)\n",
        "\n",
        "    messages = extract_triplets_chat_templ.format_messages(max_knowledge_triplets=num_triplets,\n",
        "                                                        text=text)\n",
        "\n",
        "    resp = Settings.llm.chat(\n",
        "                            messages=messages,\n",
        "                            temperature=0.0,\n",
        "                            max_tokens=16000,\n",
        "                            response_format={ \"type\": \"json_object\" },\n",
        "                            timeout = httpx.Timeout(500)\n",
        "                            )\n",
        "\n",
        "    data_dict = json.loads(resp.message.content)\n",
        "    nodes = [{\"subject\":t[\"subject\"][\"name\"], \"predicate\":t[\"predicate\"], \"object\":t[\"object\"][\"name\"]} for t in data_dict[\"triplets\"]]\n",
        "\n",
        "    descriptions = {t[\"subject\"][\"name\"]:t[\"subject\"][\"description\"] for t in data_dict[\"triplets\"]}\n",
        "    descriptions.update({t[\"object\"][\"name\"]:t[\"object\"][\"description\"] for t in data_dict[\"triplets\"]})\n",
        "\n",
        "    return nodes, descriptions"
      ],
      "metadata": {
        "id": "vwh87eReWOA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_similar_graph_nodes(description, name, edit_distance_threshold=2, max_edit_distance_based = 5, topk_vectors=5):\n",
        "    similar_names = graph_db.get_all_node_names()\n",
        "    print(similar_names)\n",
        "\n",
        "    close_matches = []\n",
        "    for similar_name in similar_names:\n",
        "        if nltk.edit_distance(similar_name, name) <= edit_distance_threshold:\n",
        "            close_matches.append(similar_name)\n",
        "\n",
        "    shuffle(close_matches)\n",
        "    if len(close_matches) > max_edit_distance_based:\n",
        "        close_matches = close_matches[:max_edit_distance_based]\n",
        "\n",
        "    asdf = vec_db.search_entity_by_description(description, top_k=topk_vectors)\n",
        "    for obj in asdf:\n",
        "        close_matches.append(obj[\"name\"])\n",
        "\n",
        "    return close_matches\n",
        "\n",
        "def llm_refine_triplet(src, dst, relationship, src_desc, dst_desc, text):\n",
        "    similar_src = find_similar_graph_nodes(src_desc, src)\n",
        "    similar_dst = find_similar_graph_nodes(dst_desc, dst)\n",
        "\n",
        "    similars = similar_src + similar_dst\n",
        "\n",
        "    descriptions = []\n",
        "    to_remove = []\n",
        "    for s in similars:\n",
        "        if sd := graph_db.get_description_by_name(s) is not None:\n",
        "            descriptions.append(sd)\n",
        "        else:\n",
        "            to_remove.append(s)\n",
        "\n",
        "    for r in to_remove:\n",
        "        similars.remove(r)\n",
        "\n",
        "    data = {}\n",
        "    data[\"document\"] = text\n",
        "    data[\"triplet\"] = {\"subject\": src, \"predicate\": relationship, \"object\": dst}\n",
        "    data[\"new_nodes\"] = [{\"entity\": src, \"description\": src_desc}, {\"entity\": dst, \"description\": dst_desc}]\n",
        "    data[\"existing_nodes\"] = [{\"entity\": similar, \"description\": description} for similar, description in zip(similars, descriptions)]\n",
        "\n",
        "    datatxt = json.dumps(data, ensure_ascii=False)\n",
        "\n",
        "    datatxt = embedding_tokenizer.restrict(datatxt, 120000)\n",
        "\n",
        "    messages = refine_triplet_chat_templ.format_messages(datajson=datatxt)\n",
        "    resp = Settings.llm.chat(messages=messages,\n",
        "                            temperature=0.0,\n",
        "                            max_tokens=16000,\n",
        "                            response_format={ \"type\": \"json_object\" }\n",
        "                            )\n",
        "\n",
        "    data = json.loads(resp.message.content)\n",
        "\n",
        "    triplet_dict = {\"subject\": data[\"refined_triplet\"][\"subject\"][\"name\"],\n",
        "                    \"predicate\": data[\"refined_triplet\"][\"predicate\"],\n",
        "                    \"object\": data[\"refined_triplet\"][\"object\"][\"name\"]}\n",
        "\n",
        "    description_dict = {data[\"refined_triplet\"][\"subject\"][\"name\"]: data[\"refined_triplet\"][\"subject\"][\"description\"],\n",
        "                        data[\"refined_triplet\"][\"object\"][\"name\"]: data[\"refined_triplet\"][\"object\"][\"description\"]}\n",
        "\n",
        "    return triplet_dict, description_dict\n",
        "\n"
      ],
      "metadata": {
        "id": "Cl2cJr13_nnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_triplets = 10"
      ],
      "metadata": {
        "id": "GXALQ07IvSXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building graph\n",
        "\n",
        "Be aware that this cell is running for a longer time. for 60 Chunks it takes around 23-25 mins."
      ],
      "metadata": {
        "id": "Yf87EO5eC7vI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for i, doc in enumerate(docs):\n",
        "    try:\n",
        "        print(f\"Processing chunk {i}\\n from\", len(docs))\n",
        "        # extracting triplets\n",
        "        triplets, descriptions = extract_triplets(max_triplets, doc.text)\n",
        "        print(f\"Extracted {len(triplets)} triplets from the chunk\")\n",
        "        for triplet in triplets:\n",
        "            try:\n",
        "                # refine triplets based on the previously processed nodes\n",
        "                final_triplet, final_descriptions = llm_refine_triplet(triplet[\"subject\"], triplet[\"object\"], triplet[\"predicate\"],\n",
        "                                descriptions[triplet[\"subject\"]], descriptions[triplet[\"object\"]], doc.text)\n",
        "                # update entities both in graph and vector database\n",
        "\n",
        "\n",
        "\n",
        "                subj_desc, subj = final_descriptions[final_triplet[\"subject\"]].replace(\"'\", \" \"), final_triplet[\"subject\"].replace(\"'\", \" \")\n",
        "                obj_desc, obj = final_descriptions[final_triplet[\"object\"]].replace(\"'\", \" \"), final_triplet[\"object\"].replace(\"'\", \" \")\n",
        "                predi = final_triplet[\"predicate\"].replace(\"'\", \" \")\n",
        "\n",
        "                subj_vid, obj_vid = graph_db.upsert_entity_relationship(subj, subj_desc,obj, obj_desc, predi)\n",
        "\n",
        "                vec_db.upsert_entity(subj, subj_desc, subj_vid, doc.text)\n",
        "                vec_db.upsert_entity(obj, obj_desc, obj_vid, doc.text)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing triplet {triplet}: {e}\")\n",
        "                print(\"Skipping...\")\n",
        "                continue\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing chunk {i}: {e}\")\n",
        "        print(\"Skipping...\")\n",
        "        continue\n",
        "\n",
        "print(\"Graph created successfully\")\n"
      ],
      "metadata": {
        "id": "ffiQf-FTT3zq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function for rendering the graph based on nodes, edges and source_nodes (which are the nodes got from the vector similarity search from the query)\n",
        "\n",
        "Here is the [Documentation for Pyvis](https://pyvis.readthedocs.io/en/latest/documentation.html)"
      ],
      "metadata": {
        "id": "iQPaPxYLDVLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_precise_timestamp():\n",
        "    # Get the current datetime\n",
        "    now = datetime.now()\n",
        "    # Format the timestamp with microseconds\n",
        "    formatted_timestamp = now.strftime(\"%Y-%m-%d-%H-%M-%S-%f\")\n",
        "    return formatted_timestamp[:-3]  # Truncate to milliseconds"
      ],
      "metadata": {
        "id": "JKDWzAnjibES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def create_renderer(edge_dict, node_dict, source_name_list = None):\n",
        "    g = Network(\n",
        "        notebook=True,\n",
        "        directed=True,\n",
        "        cdn_resources=\"in_line\",\n",
        "        height=\"700px\",\n",
        "        width=\"100%\",\n",
        "    )\n",
        "\n",
        "    for k, v in node_dict.items():\n",
        "        name = v[\"props\"][\"name\"]\n",
        "        description = v[\"props\"][\"description\"]\n",
        "        color=\"blue\"\n",
        "        if source_name_list is not None:\n",
        "            if name in source_name_list:\n",
        "                color = \"green\"\n",
        "\n",
        "        g.add_node(k, label=name, color=color, title=description)\n",
        "\n",
        "    for k, v in edge_dict.items():\n",
        "        src = v[\"src\"]\n",
        "        dst = v[\"dst\"]\n",
        "        relationship = v[\"props\"][\"relationship\"]\n",
        "        g.add_edge(src, dst, label=relationship)\n",
        "\n",
        "\n",
        "    g.repulsion(\n",
        "        node_distance=300,\n",
        "        central_gravity=0.13,\n",
        "        spring_length=250,\n",
        "        spring_strength=0.1,\n",
        "        damping=0.12,\n",
        "    )\n",
        "\n",
        "    return g\n",
        "\n",
        "def render_graph(edge_dict, node_dict, source_name_list = None):\n",
        "    g = create_renderer(edge_dict, node_dict, source_name_list)\n",
        "    time_stamp = get_precise_timestamp()\n",
        "    g.show(f\"graph_{time_stamp}.html\")\n",
        "    return HTML(filename=f\"graph_{time_stamp}.html\")"
      ],
      "metadata": {
        "id": "EzD-JX8q1Org"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fm-Oy_ZdZ2bN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "node_dict, edge_dict = graph_db.get_full_graph()\n",
        "render_graph(edge_dict, node_dict)"
      ],
      "metadata": {
        "id": "uZ-eGqRlRVNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extracts entities with descriptions from the user query and a hypothetical answer.\n",
        "Here, the LLM (used in JSON mode) is one-shot prompted to format the last user message into a standalone question that is general enough and resolves ambiguities and references using the chat history. Then the model should extract at most 5 entities (node candidates) from the question and provide 'hypothetical' descriptions for them. Finally a hypothetical general answer is generated which is not used in the current implementation but if someone wants to extend this retrieving process, you can include the hypothetical answer as well. :D"
      ],
      "metadata": {
        "id": "cuzKwQJND-6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def form_question(chat_memory):\n",
        "    \"\"\" Return a question, HyDE-like hypothetical answer and search phrases with hypothetical descriptions based on the chat history.\n",
        "\n",
        "    Args:\n",
        "        user_message: The user's message.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the question, hypothetical answer and search phrases with hypothetical descriptions\n",
        "    \"\"\"\n",
        "\n",
        "    history_dict = [{\"role\":m.role, \"content\":m.content} for m in chat_memory.get_all()]\n",
        "\n",
        "    history_json = {\"chat_history\": history_dict}\n",
        "    messages = question_forming_chat_templ.format_messages(historyjson=json.dumps(history_json, ensure_ascii=False),\n",
        "                                                           max_entities=5)\n",
        "    resp = Settings.llm.chat(messages=messages,\n",
        "                            temperature=0.0,\n",
        "                            max_tokens=16000,\n",
        "                            response_format={ \"type\": \"json_object\" }\n",
        "                            )\n",
        "\n",
        "    data = json.loads(resp.message.content)\n",
        "\n",
        "    return data[\"question\"], data[\"hypothetical_answer\"], data[\"phrases\"]\n"
      ],
      "metadata": {
        "id": "BOK6UxvHDz66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieving daata based on the extracted entities from the user query and chat history\n",
        "\n",
        "The retrieval procedure first selects \"source nodes\" that fulfill at least one of the following criteria:   \n",
        " (1) The name of the entity matches a graph node's name closely.   \n",
        " (2) The hypothetical embedding is similar to a graph node.   \n",
        "Rule (1) is checked using exact matching while for rule (2) we execute a classic retrieval on the vector database returning the top 5 most similar nodes based on their description for each entity extracted during query forming.   \n",
        "Finally the graph is queried, we extract all triplets from the graph within 2 hops from any source node we located. The triplets and the visited nodes are stored in a string format. Extended with text chunks that correspond to source nodes only (we cannot afford to pass all chunks :D) we construct the context for the LLM."
      ],
      "metadata": {
        "id": "k-ELqH6UEvVJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_graph_data(phrases, hypothetical_answer):\n",
        "    \"\"\" Retrieve graph data based on the search phrases, their descriptions and the hypothetical answer.\n",
        "\n",
        "    Args:\n",
        "        phrases (list(dict(str, str)): A list of dictionaries containing the search phrases and their hypothetical descriptions in the form of {\"entity\": str, \"description\": str}.\n",
        "        hypothetical_answer (str): The hypothetical answer to the question.\n",
        "\n",
        "    Returns:\n",
        "        str: The context containing the related document chunks, nodes and connections.\n",
        "        dict: A dictionary containing the node data.\n",
        "        dict: A dictionary containing the edge data.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    related_texts = []\n",
        "    source_nodes = []\n",
        "\n",
        "    node_names = graph_db.get_all_node_names()\n",
        "\n",
        "    for phrase in phrases:\n",
        "        vec_data = vec_db.search_entity_by_description(phrase[\"description\"], top_k=5)\n",
        "\n",
        "        for obj in vec_data:\n",
        "            related_texts +=obj[\"texts\"]\n",
        "            source_nodes.append(obj[\"name\"])\n",
        "\n",
        "        if phrase[\"entity\"] in node_names:\n",
        "            source_nodes.append(phrase[\"entity\"])\n",
        "\n",
        "    source_nodes = list(set(source_nodes))\n",
        "    related_texts = list(set(related_texts))\n",
        "\n",
        "    related_descriptions = {}\n",
        "    for node in source_nodes:\n",
        "        related_descriptions[node] = graph_db.get_description_by_name(node)\n",
        "\n",
        "    all_node_data, all_edge_data = graph_db.get_all_data_in_hops(source_nodes, hops=2)\n",
        "\n",
        "    for k, v in all_node_data.items():\n",
        "        related_descriptions[v[\"props\"][\"name\"]] = v[\"props\"][\"description\"]\n",
        "\n",
        "\n",
        "    related_description_strs = []\n",
        "    for name, description in related_descriptions.items():\n",
        "        if name is not None:\n",
        "            if description is not None:\n",
        "                related_description_strs.append(\"- \" + name + \": \" + description + \"\\n\")\n",
        "            else:\n",
        "                related_description_strs.append(\"- \" + name + \"\\n\")\n",
        "\n",
        "    related_description_strs = list(set(related_description_strs))\n",
        "\n",
        "    edge_strs = []\n",
        "    for k, v in all_edge_data.items():\n",
        "        edge_strs.append(\"(\"+all_node_data[v[\"src\"]]['props'][\"name\"]+\")\"+\"--\"+\"[\"+v[\"props\"][\"relationship\"]+\"]\"+\"-->\"+\"(\"+all_node_data[v[\"dst\"]]['props'][\"name\"]+\")\")\n",
        "\n",
        "    related_connections = list(set(edge_strs))\n",
        "\n",
        "    related_connections, conn_tok_num = chat_tokenizer.restrict(\"\\n\".join(related_connections), 80000, return_len=True)\n",
        "    related_description_strs, desc_tok_num = chat_tokenizer.restrict(\"\\n\".join(related_description_strs), 80000-conn_tok_num, return_len=True)\n",
        "    related_texts = chat_tokenizer.restrict(\"\\n\\n\".join(related_texts), 80000-conn_tok_num-desc_tok_num)\n",
        "\n",
        "    if related_connections == \"\" or related_connections is None:\n",
        "        related_connections = \"No related connections found.\"\n",
        "\n",
        "    if related_description_strs == \"\" or related_description_strs is None:\n",
        "        related_description_strs = \"No related nodes found.\"\n",
        "\n",
        "    if related_texts == \"\" or related_texts is None:\n",
        "        related_texts = \"No related document chunks found.\"\n",
        "\n",
        "    context = \"Related Connections:\\n\"+related_connections+\"\\n\\n\\n\"+\\\n",
        "              \"Related Nodes:\\n\"+related_description_strs+\"\\n\\n\\n\"+\\\n",
        "              \"Related Document Chunks:\\n\"+related_texts\n",
        "\n",
        "    if len(source_nodes) == 0:\n",
        "        source_nodes = None\n",
        "\n",
        "    return context, all_node_data, all_edge_data, source_nodes"
      ],
      "metadata": {
        "id": "u3VwaE3UJcj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Asking question from the model with and without using context information\n",
        "Here you can type an input question about the PDF, and it will query the model without any context, and with the context retrieved with our retrieval sysrtem."
      ],
      "metadata": {
        "id": "aPJ2R6QLFdfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AnswerGeneration:\n",
        "  def __init__(self, starter_msg=\"Welcome!\\n Ask, and I will answer based on the knowledge graph!\"):\n",
        "        self.prefix_context = bot_prefix_chat_templ\n",
        "        self.prefix = bot_prefix_chat_without_context_templ\n",
        "        self.starter_msg = starter_msg\n",
        "\n",
        "  def answer_with_context(self, user_message):\n",
        "      history = ChatMemoryBuffer.from_defaults(token_limit=32000)\n",
        "      history.put(ChatMessage(content=user_message, role=\"user\"))\n",
        "\n",
        "      question, hypothetical_answer, phrases = form_question(history)\n",
        "      if phrases is not None:\n",
        "          context, node_dict, edge_dict, source_node_names = retrieve_graph_data(phrases, hypothetical_answer)\n",
        "      messages = self.prefix_context.format_messages(context_info=context, starter_message = self.starter_msg)\n",
        "      messages.append(ChatMessage(content=user_message, role=\"user\"))\n",
        "      resp = Settings.llm.chat(messages=messages,\n",
        "                              temperature=0.4,\n",
        "                              max_tokens=3000)\n",
        "\n",
        "      generated_answer = resp.message.content\n",
        "\n",
        "      return generated_answer, node_dict, edge_dict, source_node_names\n",
        "\n",
        "\n",
        "  def answer_without_context(self, user_message):\n",
        "      messages = self.prefix.format_messages(starter_message = \"\")\n",
        "      messages.append(ChatMessage(content=user_message, role=\"user\"))\n",
        "      resp = Settings.llm.chat(messages=messages,\n",
        "                              temperature=0.4,\n",
        "                              max_tokens=3000)\n",
        "      generated_answer = resp.message.content\n",
        "      return generated_answer"
      ],
      "metadata": {
        "id": "Cdtl-4qCWDRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_newlines(text, line_length=100):\n",
        "    \"\"\"\n",
        "    Add a newline character after every specified number of characters efficiently.\n",
        "\n",
        "    Parameters:\n",
        "    text (str): The input text to be formatted.\n",
        "    line_length (int): The number of characters after which to insert a newline.\n",
        "\n",
        "    Returns:\n",
        "    str: The formatted text with newlines added.\n",
        "    \"\"\"\n",
        "    if line_length <= 0:\n",
        "        return text\n",
        "\n",
        "    segments = []\n",
        "    for i in range(0, len(text), line_length):\n",
        "        segments.append(text[i:i + line_length])\n",
        "\n",
        "    return '\\n'.join(segments)"
      ],
      "metadata": {
        "id": "N2CM1E7xj7tv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer_engine = AnswerGeneration()\n",
        "\n",
        "# Example question: What do you know about OOM?\n",
        "\n",
        "question = input(\"Enter your question: \")  # Get question from the input\n",
        "\n",
        "response_no_context = answer_engine.answer_without_context(question)\n",
        "print(f\"GPT answer without context \\n\\n{add_newlines(response_no_context)}\\n\\n\")\n",
        "\n",
        "print(\"Generating answer with context...\")\n",
        "\n",
        "response, node_dict, edge_dict, source_node_names = answer_engine.answer_with_context(question)\n",
        "print(f\"GPT answer with context: \\n\\n{add_newlines(response)}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEiZhyiXUXj5",
        "outputId": "24c3816b-0ca0-47b5-f721-bfe69bd5d855"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your question: What do you know about OOM?\n",
            "GPT answer without context \n",
            "\n",
            "OOM stands for \"Out of Memory,\" which refers to a condition in computing where a program or process \n",
            "runs out of memory resources to continue execution. This can lead to crashes, slow performance, or t\n",
            "he inability to allocate memory for new tasks. OOM can occur in various environments, including oper\n",
            "ating systems, applications, and virtual machines.\n",
            "\n",
            "\n",
            "Generating answer with context...\n",
            "GPT answer with context: \n",
            "\n",
            "OOM stands for \"Order of Magnitude,\" which is often used to describe a significant change or improve\n",
            "ment in a measurable quantity. In the context provided, OOM is related to training efficiency gains \n",
            "and is associated with the performance improvements that automated AI researchers aim to achieve. It\n",
            " indicates the potential for advancements that could lead to five orders of magnitude in performance\n",
            " improvement, although such a level may be unattainable. Additionally, OOM is connected to the conce\n",
            "pt of compute bottlenecks, which are limitations in computational resources that restrict the abilit\n",
            "y to conduct extensive experiments.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "render_graph(edge_dict, node_dict, source_node_names)"
      ],
      "metadata": {
        "id": "uKlcl3RNZzZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "okCoU8Hrfm7_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chatting with model about the PDF\n",
        "\n",
        "The answer is generated in a zero-shot manner with the context in the system prompt and up to 32k conversation history tokens. The LLM (in text mode) is prompted with the system prompt, and any history that fits into this 32k limit. Then it generates the answer which is returned to the user. The typical response time is around 10-20 seconds for non-query responses and 30-60 seconds for query responses.   \n",
        "The graph is visualized with source nodes (green) and connected nodes (blue) under the details spoiler of the last response. Prevously generated graph visualizations are not stored, only a single last graph is available (including the full graph visualization)"
      ],
      "metadata": {
        "id": "FDCTIVzoGsPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ChatEngine:\n",
        "    def __init__(self, starter_msg=\"Welcome!\\n Ask, and I will answer based on the knowledge graph!\"):\n",
        "        self.memory = ChatMemoryBuffer.from_defaults(token_limit=32000)\n",
        "        self.prefix = bot_prefix_chat_templ\n",
        "        self.starter_msg = starter_msg\n",
        "\n",
        "    def generate_answer(self, user_message, context):\n",
        "        messages = self.prefix.format_messages(context_info=context, starter_message = self.starter_msg)\n",
        "\n",
        "        memory_messages = self.memory.get_all()\n",
        "\n",
        "        messages += memory_messages\n",
        "\n",
        "        messages.append(ChatMessage(content=user_message, role=\"user\"))\n",
        "\n",
        "        resp = Settings.llm.chat(messages=messages,\n",
        "                                temperature=0.4,\n",
        "                                max_tokens=3000)\n",
        "\n",
        "        return resp.message.content\n",
        "\n",
        "    def chat_one_turn(self, user_message):\n",
        "        self.memory.put(ChatMessage(content=user_message, role=\"user\"))\n",
        "        question, hypothetical_answer, phrases = form_question(self.memory)\n",
        "        if phrases is not None:\n",
        "            context, node_dict, edge_dict, source_node_names = retrieve_graph_data(phrases, hypothetical_answer)\n",
        "        else:\n",
        "            context = \"\"\n",
        "            node_dict = {}\n",
        "            edge_dict = {}\n",
        "            source_node_names = None\n",
        "\n",
        "        generated_answer = self.generate_answer(user_message, context)\n",
        "        self.memory.put(ChatMessage(content=generated_answer, role=\"assistant\"))\n",
        "        return generated_answer, node_dict, edge_dict, source_node_names\n",
        "\n",
        "    def reset(self):\n",
        "        self.memory.reset()\n"
      ],
      "metadata": {
        "id": "IDxQDnReAlub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KJNPro4cjyy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_engine = ChatEngine()\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"Enter your message: \\n\")\n",
        "    if not user_input:\n",
        "        break  # Exit if input is empty\n",
        "\n",
        "    # Process chat and get graph data\n",
        "    response, node_dict, edge_dict, source_node_names = chat_engine.chat_one_turn(user_input)\n",
        "\n",
        "    # Visualize the graph if data exists\n",
        "    if node_dict and edge_dict:\n",
        "        print(f\"Assistant: {add_newlines(response)} \\n\")\n",
        "        render_graph(edge_dict, node_dict, source_node_names)\n",
        "    else:\n",
        "        print(f\"Assistant: {response} \\n\")\n"
      ],
      "metadata": {
        "id": "xF-_wXTRHScF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WVMX7NWjhMRQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
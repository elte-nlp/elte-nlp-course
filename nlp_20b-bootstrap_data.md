---
title: "Natural Language Processing"
subtitle: "Lecture 20b: Extension: Bootstrapping Data"
author: "Natabara Gyöngyössy"
institute: "Eötvös University, Department of Artificial Intelligence"
date: 2023
theme: Marburg
colortheme: orchid
fontsize: 12pt
linkcolor: blue
header-includes: |
  \let\emphasized\emph
  \let\strong\textbf
  \renewcommand{\textbf}[1]{\textcolor{blue}{\strong{#1}}}
  `\setbeamertemplate{navigation symbols}{}`{=latex}
  `\setbeamertemplate{footline}[page number]`{=latex}
link-citations: true
---
# Bootstrapping

## What is Bootstrapping?

**Bootstrapping** is a method for using existing resources to create new ones. 

In our case, we are going to use the existing pre-trained models to create new datasets for training new models.
We typically use the largest, best performing available models. In late-$2023$ these models are GPT-4 for private and LLaMa2-70B for open models.

Bootstrapping is:

- Cost-effective
- Fast
- Easy to implement
- Able to generate high-complexity data
- Risky (license issues, quality issues)

# Data Generation with Existing Models

## "Self"-Instruct

Here we use the existing model to generate data for our own model. In this case the other model is the teacher, and our model is the student.

**Important difference**: Opposed to distillation, we do not use the teacher's predictions on the vector level, but rather the teacher's output on the token level in the dataset. This way direct access is not needed to the teacher model.

Stanford Alpaca [@alpaca] claims that this way instruct fine-tuning is cost-effective and fast. It can be accomplished in a few hundred dollars.

![](figures/alpaca_logo.png){width=50%}

## Zero-Shot Chain-of-Thought

Reliable chain-of-thought (CoT) prompting [@weng2023prompt] needs a few examples to work. By using CoT prompts we can generate a large dataset of CoT completions for a given topic.

WizardLM [@xu2023wizardlm] utilizes a more abstract method for instruction generation by using *Evol-Instruct* to step-by-step evolve the instructions for a given task. This way the instructions that are generated will cover a wider range of the task's space with more complex prompts.

We prompt our LLM to generate modified versions of the instructions, and then use these to generate the dataset. These modificaiton steps can be chained.

## Evol-Instruct

Examples for task evolution (for base task "1+1=?"):

\small
- Deepening: In what situation does 
1+1 not equal to 2?
- Incrase reasoning: What is the value of x, if x^3 + 2x + 3 = 7?
- Concretizing: If you have one apple and someone 
gives you another banana, how 
many fruits do you have?
- Add constraint: How to prove 1 + 1 = 2 in 
the Goldbach Conjecture?
- Complicate Input: 1/(sqrt(2) + 4^2) = ?
- In-Breadth Evolving: What is the speed of light in a vacuum?
- Increase reasoning of Evolved (above): How many times faster is light 
than sound in a vacuum?

## Evolution steps

![Figure from [@xu2023wizardlm]](figures/evolinstruct.png){height=90%}

## Removing evolution

Elimination of intermediate results happens when:

1. The evolved instruction does not provide any information gain compared to the original one. ChatGPT is used to make this determination.
2. The evolved instruction makes it difficult for the LLM to generate a response. If the generated response contains “sorry” and is relatively short in length (i.e., less than 80 words), it often indicates that the LLM struggles to respond to the evolved instruction.
3. The response generated by the LLM only contains punctuation and stop words.
4. The evolved instruction obviously copies some words from the evolving prompt, such as “given prompt”, “rewritten prompt”, “#Rewritten Prompt#”, etc.

## Effect of EvolInstruct

EvolInstruct fine-tuning is able to improve performance on high-complexity tasks as shown in the figure below.

![Figure from [@xu2023wizardlm]](figures/wizard_results.png){height=70%}

## Orca

EvolInstruct introduced variety in the instruction generation side. Opposed to this Orca [@mukherjee2023orca] dives into the response generation side. Especially the reasoing and explanation generation. In the original paper they define various system prompts for the LLM, that guide the response generation style.

Some examples include:

- You are an AI assistant. Provide a detailed answer so user don’t need to search outside to
understand the answer.
- You should describe the task and explain your answer. While answering a multiple choice
question, first output the correct answer(s). Then explain why other answers are wrong.
Think like you are answering to a five year old.

## Advantages of Explanation Tuning

Hard and professional tasks are easily solved by small models that are tuned with explanations.

![Figure from [@mukherjee2023orca]](figures/orca_results.png){height=65%}

## Advantages of Explanation Tuning

Hard and professional tasks are easily solved by small models that are tuned with explanations.

![Figure from [@mukherjee2023orca]](figures/orca_results2.png){height=65%}

## Model Evaluation

Evaluating complex models is hard, as there is no clear way to assess open-domain performance. Common methods include human and LLM judges.

Human judges are expensive and slow, but crowd-sourcing can be used to speed up and stabilize the process, such is the case with Chatbot Arena [@zheng2023judging]. Chatbot Arena is a platform for evaluating chatbots, where the users chat with multiple bots and denote their preferences.

LLM judges are faster and cheaper but more biased. Utilization methods include: Pairwise comparison of two answers, Single answer grading (score assignment), Reference-guided grading (score assignment).

## Model bias

According to @zheng2023judging the LLM judges are biased towards the first answer, as well as the longer answer. It is worth to use symmetric evaluation. "Rename" prompts demonstrate that some models (Claude-v1) are also biased towards names (such as Assistant A, Assistant B, etc.).

\footnotesize
| Judge      |   Prompt  | Consistency | Biased <br> toward <br> first | Biased <br> toward <br> second | Error |
|------------|------------|--------------|--------|--------|-------|
| Claude-v1  |  default   |  23.8%       | 75.0%  | 0.0%   | 1.2%  |
| Claude-v1  |  rename    |  56.2%       | 11.2%  | 28.7%  | 3.8%  |
| GPT-3.5    |  default   |  46.2%       | 50.0%  | 1.2%   | 2.5%  |
| GPT-3.5    |  rename    |  51.2%       | 38.8%  | 6.2%   | 3.8%  |
| GPT-4      |  default   |  65.0%       | 30.0%  | 5.0%   | 0.0%  |
| GPT-4      |  rename    |  66.2%       | 28.7%  | 5.0%   | 0.0%  |



# References

## References {.allowframebreaks} 
\footnotesize
